<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />

  <!-- BEGIN Info -->
  <meta name="description"
    content="Waypost - An open-source feature flag management system that specializes in A/B Testing" />
  <meta name="title" property="og:title" content="Waypost" />
  <meta property="og:type" content="Website" />
  <meta name="image" property="og:image" content="images/thumb.png" />
  <meta name="description" property="og:description"
    content="Waypost - An open-source feature flag management system that specializes in A/B Testing" />
  <meta name="author" content="Waypost" />
  <!-- END Info -->
  <!-- BEGIN favicon -->
  <link rel="apple-touch-icon" sizes="180x180" href="images/favicon/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="images/favicon/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="images/favicon/favicon-16x16.png" />
  <link rel="manifest" href="images/favicon/site.webmanifest" />
  <link rel="shortcut icon" href="images/favicon/favicon.ico" />
  <meta name="msapplication-TileColor" content="#ffffff" />
  <meta name="msapplication-config" content="images/favicon/browserconfig.xml" />
  <meta name="theme-color" content="#ffffff" />
  <!-- END favicon -->

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Waypost</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" />
  <link rel="stylesheet" href="stylesheets/reset.css" />
  <link rel="stylesheet" href="stylesheets/style.css" />
  <link rel="stylesheet" href="stylesheets/responsive.css" />
</head>

<body>
  <header class="mobile-menu-closed">
    <div id="header">
      <a href="/">
        <img src="images/logo/Waypost_graphic_color.svg" />
        <h1>Waypost</h1>
      </a>
      <nav>
        <a href="#start-here" class="selected">Start Here</a>
        <a href="#case-study">Case Study</a>
        <a href="#presentation">Presentation</a>
        <a href="#our-team">Our Team</a>
        <a href="/documentation">Docs</a>
        <a href="https://github.com/waypost-io" target="_blank" class="icon"><i class="fab fa-github"></i></a>
      </nav>
      <div id="menu">
        <button type="button">
          <svg id="mobile-open" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"
            aria-hidden="true">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16" />
          </svg>
          <svg id="mobile-close" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"
            stroke="currentColor" aria-hidden="true">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
          </svg>
        </button>
      </div>
    </div>
    <div id="header-buffer"></div>
    <div id="mobile-menu">
      <a href="#start-here" class="selected">Start Here</a>
      <a href="#case-study">Case Study</a>
      <a href="#presentation">Presentation</a>
      <a href="#our-team">Our Team</a>
      <a href="/documentation">Docs</a>
      <a href="https://github.com/waypost-io" target="_blank"><i class="fab fa-github"></i> GitHub</a>
    </div>
  </header>

  <div id="start-here" class="main-section">
    <div class="h-full">
      <div class="bg-offwhite static-logo-color"></div>
      <div class="bg-black">
        <h4 id="hero-text">
          <span class="text-skyblue">Waypost</span> is an open-source <span class="text-skyblue">feature flagging</span>
          platform specializing in <span class="text-skyblue">A/B testing</span>
        </h4>
      </div>
    </div>
    <div class="h-full">
      <div class="bg-violet static-logo-dark">
        <h2>Easily Toggle and Test New Features</h2>
      </div>
      <div class="bg-violet">
        <h2 class="sm-header">Easily Toggle and Test New Features</h2>
        <p>
          Manage feature flags and experiments with a couple clicks.
        </p>
        <p>Waypost analyzes the data and provides experiment results.</p>
        <img src="./images/diagrams/5.2_flags_dashboard.gif" alt="Flags Dashboard" />
      </div>
    </div>
    <div class="h-full">
      <div class="bg-turquoise static-logo-dark">
        <h2>Customizable and Flexible</h2>
      </div>
      <div class="bg-turquoise">
        <h2 class="sm-header">Modular and Flexible</h2>
        <p>Waypost comes Dockerized for simple deployment.</p>
        <p>Completely open-source and self-hosted, so it can be customized according to your needs.</p>
        <p>Provides real-time updates and is easily scalable.</p>
        <img src="./images/diagrams/architecture_with_bg.png" alt="Architecture" />
      </div>
    </div>
  </div>

  <aside id="toc">
    <ul>
      <!-- Section 1 -->
      <li data-section="section-1" class="selected">
        <a href="#section-1">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Introduction</p>
          </div>
        </a>
      </li>
      <li data-section="section-1" class="subitem">
        <a href="#section-1-1">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Hypothetical Waypost User</p>
          </div>
        </a>
      </li>
      <li data-section="section-1" class="subitem">
        <a href="#section-1-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Comparing Two Versions</p>
          </div>
        </a>
      </li>
      <!-- Section 2 -->
      <li data-section="section-2">
        <a href="#section-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>A/B Testing</p>
          </div>
        </a>
      </li>
      <li data-section="section-2" class="subitem">
        <a href="#section-2-1">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>What is A/B Testing</p>
          </div>
        </a>
      </li>
      <li data-section="section-2" class="subitem">
        <a href="#section-2-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Interpreting Results</p>
          </div>
        </a>
      </li>
      <li data-section="section-2" class="subitem">
        <a href="#section-2-3">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Challenges of A/B Testing</p>
          </div>
        </a>
      </li>
      <!-- Section 3 -->
      <li data-section="section-3">
        <a href="#section-3">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Implementing Two Versions of a Site</p>
          </div>
        </a>
      </li>
      <li data-section="section-3" class="subitem">
        <a href="#section-3-1">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Multiple Deployments</p>
          </div>
        </a>
      </li>
      <li data-section="section-3" class="subitem">
        <a href="#section-3-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Feature Flags</p>
          </div>
        </a>
      </li>
      <!-- Section 4 -->
      <li data-section="section-4">
        <a href="#section-4">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Existing Solutions</p>
          </div>
        </a>
      </li>
      <li data-section="section-4" class="subitem">
        <a href="#section-4-1">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>DIY</p>
          </div>
        </a>
      </li>
      <li data-section="section-4" class="subitem">
        <a href="#section-4-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Paid Solutions</p>
          </div>
        </a>
      </li>
      <!-- Section 5 -->
      <li data-section="section-5">
        <a href="#section-5">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Waypost</p>
          </div>
        </a>
      </li>
      <li data-section="section-5" class="subitem">
        <a href="#section-5-1">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Architecture Overview</p>
          </div>
        </a>
      </li>
      <li data-section="section-5" class="subitem">
        <a href="#section-5-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Manager App</p>
          </div>
        </a>
      </li>
      <li data-section="section-5" class="subitem">
        <a href="#section-5-3">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Flag Provider Service</p>
          </div>
        </a>
      </li>
      <li data-section="section-5" class="subitem">
        <a href="#section-5-4">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>SDK</p>
          </div>
        </a>
      </li>
      <li data-section="section-5" class="subitem">
        <a href="#section-5-5">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>How Flag Data is Sent From the Manager to SDKs</p>
          </div>
        </a>
      </li>
      <li data-section="section-5" class="subitem">
        <a href="#section-5-6">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>A/B Testing with Waypost</p>
          </div>
        </a>
      </li>
      <!-- Section 6 -->
      <li data-section="section-6">
        <a href="#section-6">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Engineering Decisions</p>
          </div>
        </a>
      </li>
      <li data-section="section-6" class="subitem">
        <a href="#section-6-1">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Hosted vs. Self-Hosted</p>
          </div>
        </a>
      </li>
      <li data-section="section-6" class="subitem">
        <a href="#section-6-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Collecting User Event Data</p>
          </div>
        </a>
      </li>
      <li data-section="section-6" class="subitem">
        <a href="#section-6-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Communication Between Manager App and Flag Provider</p>
          </div>
        </a>
      </li>
      <li data-section="section-6" class="subitem">
        <a href="#section-6-3">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Providing Feature Flag Data to Client</p>
          </div>
        </a>
      </li>
      <li data-section="section-6" class="subitem">
        <a href="#section-6-4">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Providing Feature Flag Data to the SDK</p>
          </div>
        </a>
      </li>
      <li data-section="section-6" class="subitem">
        <a href="#section-6-5">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Statistics Pipeline</p>
          </div>
        </a>
      </li>
      <!-- Section 7 -->
      <li data-section="section-7">
        <a href="#section-7">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Future Work</p>
          </div>
        </a>
      </li>
      <li data-section="section-7" class="subitem">
        <a href="#section-7-1">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Extended Database Integration</p>
          </div>
        </a>
      </li>
      <li data-section="section-7" class="subitem">
        <a href="#section-7-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Separate Feature Flags by Application</p>
          </div>
        </a>
      </li>
      <li data-section="section-7" class="subitem">
        <a href="#section-7-3">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Login Capability</p>
          </div>
        </a>
      </li>
      <li data-section="section-7" class="subitem">
        <a href="#section-7-4">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Additional Language Support for SDKs</p>
          </div>
        </a>
      </li>
      <!-- Section 8 -->
      <li data-section="section-8">
        <a href="#section-8">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Glossary</p>
          </div>
        </a>
      </li>
      <!-- Section 9 -->
      <li data-section="section-9">
        <a href="#section-9">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>References</p>
          </div>
        </a>
      </li>
    </ul>
  </aside>

  <div id="case-study" class="main-section">
    <div id="case-study-content">
      <div class="prose prose-xl">
        <h1>Case Study</h1>
        <!-- Case study goes here -->
        <!-- Section 1 -->
        <h2 id="section-1">1. Introduction</h2>
        <p><strong>Waypost is an open-source, lightweight, self-hosted feature flag management platform that specializes in A/B
          Testing</strong>, providing analytical insights for your experiments on both the front-end and back-end.</p>
        <p>As a team of developers starts to iterate on their application and develop new features, they’ll want to have
          confidence that the choices they’re making are benefiting the business. They’ll want to collect and analyze
          the behavior of users of their application in order to gain insights on whether their choices are helping,
          hurting, or having no effect on their bottom line. They’ll also need an easy and quick way to manage these new
          features and to turn them off if something goes wrong.</p>
        <p>By using feature flags with Waypost to manage and test their new features, developers can confidently push
          new features to their applications with the knowledge that the risk of the feature hurting their business is
          minimized.</p>
        <h3 id="section-1-1">1.1 Hypothetical Waypost User</h3>
        <p>Solar Flair is a solar panel installation company with a handful of software engineers on their staff. Their
          main way of gaining business is via their website’s landing page. As is standard in the solar panel
          installation business, their primary sales funnel starts with clicking a button called “Get a Quote Today”
          which opens up a form.</p>
        <p>In Solar Flair’s first iteration of their website, the form requires the customer to input the architectural
          data and their address in order to calculate how much sunlight their location typically gets. The developer
          team at Solar Flair sets up event tracking to try and understand how people interact with their website and
          they discover that 80% of people who click the “Get a Quote Today” button never finish the form and their
          quote never gets calculated. They get lost in step one of the funnel.</p>
        <p>The developers get together and discuss why this is happening and theorize that entering all the
          architectural data is tedious and cumbersome. They believe they can increase the percentage of people signing
          up for an appointment by using a 3rd-party software which only requires the address of the home as input and
          then calculates the quote using satellite imagery and climate statistics. However, this software costs a
          subscription fee, so the team wants to be sure that this fee is worth the cost and gets them enough new
          customers to gain a profit. In addition, since it’s a 3rd-party software/API that may get updated, they don’t
          want their entire site to go down if it gets a major update and the quote calculation doesn’t work. </p>
        <img src="./images/diagrams/1.1_feature_comparison.png" alt="feature comparison" width="500px">
        <p>Solar Flair needs a way to <strong>measure and analyze user behavior</strong> when they use the new quote feature vs. the
          old one. In other words:</p>

        <ol>
          <li>They need to serve users either the new quote feature or the old one.</li>
          <li>They also need to determine whether users preferred the new quote, the old quote, or the change had no
          difference in behavior.</li>
        </ol>
        <p>If something were to go wrong with the new quote feature, Solar Flair would like all users to be served the
          old one as soon as possible to minimize any business losses from the bug while it gets fixed.</p>
        <h3 id="section-1-2">1.2 Comparing Two Versions</h3>
        <p>One option is to roll out the new feature temporarily and measure how the metrics change. But there are
          several problems with this:</p>
        <ol>
          <li>The new feature hasn’t been tested in production and could have bugs. Therefore, Solar Flair wouldn’t want it
          rolled out to everyone at once.</li>
          <li>This strategy doesn’t control for external factors, like time. For example, users may be more likely to
          sign up at different times of the year. Without controlling for external factors, the change in metrics might
          be attributed to the new feature when it was actually caused by something else.</li>
        </ol>
        <p>Therefore, Solar Flair needs to be sure the only difference between the two user experiences is the different
          feature being served to them.To do this, they’ll have to <strong>show both features during the same time period and
          randomize who gets which feature</strong>.</p>
        <p>The solution Solar Flair is looking for is experimentation, also known as <strong>A/B testing</strong>.</p>
        <h2 id="section-2">2. A/B Testing</h2>
        <h3 id="section-2-1">2.1 What is A/B Testing?</h3>
        <p>A/B testing is the practice of testing two or more versions of a feature at once, in which a random group of
          users is assigned to receive one version, and another random group of users is assigned the other version, for
          a certain period of time. The A/B test can be considered the most basic kind of <strong>randomized controlled
          experiment</strong>. In its simplest form, there are two treatments and one acts as the control for the other [1].</p>
        <img src="./images/diagrams/2.1_A_and_b_features.png" alt="A and B features" width="300px">
        <h3 id="section-2-2">2.2 Interpreting Results</h3>
        <p>After an A/B test has finished running, one can analyze the results to make a data-driven decision. Every
          experiment should have a primary <strong>metric</strong>, and possibly secondary metrics, by which it will be evaluated. The
          aggregate values for these metrics get collected for the test group and the control group for comparison. One
          can use statistics to determine if there was a statistically significant change in the metrics or not.
          <strong>Statistical significance</strong> is determined by a statistic called a “p-value”. With this information, the team can
          conclude whether the new feature being tested was “successful” or not, and make a decision whether to <strong>rollout
          the feature to their entire userbase</strong>, to <strong>remove it</strong>, or to <strong>continue to run more experiments</strong>. For example, if
          Solar Flair obtained a statistically significant increase in form completion rate with their new quote
          feature, then they could confidently roll out the new feature.</p>
        <img src="./images/diagrams/2.2_AB_decision.png" alt="A/B decision" width="1000px">
        <h3 id="section-2-3">2.3 Challenges of A/B Testing</h3>
        <p>As A/B testing requires a large amount of data to work, implementing A/B testing comes with several
          challenges. There are two primary categories that the challenges of A/B testing fall under: engineering
          challenges and statistics challenges. </p>
        <h4>2.3.1 Engineering Challenges</h4>
        <p>From an engineering standpoint, developers must contend with the challenges of: </p>
        <ul>
        <li><strong>How to assign users randomly into groups?</strong> Users must be randomly assigned so that the two groups can be
          fairly compared, and so the two groups can be roughly the same size. </li>
        <li><strong>How to keep track of what group they were in?</strong> Each user must be given the same treatment on each visit.</li>
        <li><strong>How to log user events?</strong> Each time an event that we are interested in occurs, it must be logged to the
          database along with relevant information regarding who, what, and where. In addition, one must log when a user
          is exposed to an experiment. This will result in a huge volume of writes to the database.</li>
        <li><strong>How to pull the metrics to analyze afterward?</strong> Each metric is calculated differently and some may rely on the
          same events, requiring the event data to be queryable and accessible. The metric calculation should also be
          automated so that one does not have to manually query the metrics each time.</li>
        </ul>
        <h4>2.3.2 Statistical Challenges</h4>
        <p>From a statistics standpoint, the main challenges are:</p>
        <ul>
          <li><strong>Designing the experiment correctly such that we can gain meaningful results from it.</strong> It may be surprising how
            easy it is to design an experiment incorrectly. For example, when running multiple experiments at the same
            time, one must ensure that different experiments do not influence each other’s results.</li>
          <li><strong>Determining which statistical tests to run on the data.</strong> There are many different types of statistical tests
            to choose from, each with different purposes. Therefore, picking which statistical tests to run must be done
            thoughtfully and with care.</li>
        </ul>
        <h2 id="section-3">3. Implementing Two Versions of a Site</h2>
        <p>Before Solar Flair can place their users into “test” and “control” groups, they must be able to serve the
          groups different versions of their website at the same time. There are two common ways to implement this:</p>
        <ol>
          <li>Have multiple deployments of the website running and route users to one or the other.</li>
          <li>Use feature flags to render their site one way or the other at run time.</li>
        </ol>
        <h3 id="section-3-1">3.1 Possible Solution: Multiple Deployments</h3>
        <p>This strategy is implemented by having <strong>two versions of an application running in production at once</strong>. The new
          version of the application is deployed to a set percentage of the organization’s servers. A <strong>router</strong> (usually a
          load balancer) will route users to the two sets of servers. The “<strong>rollout percentage</strong>,” the percentage of the
          users get routed to one group vs. the other, can be set to any percentage the user wants, whether it be 50/50,
          10/90, etc.</p>
        <img src="./images/diagrams/3.1_multiple_deployments.png" alt="multiple deployments">
        <p>From here, if the new version proves to be an improvement, then the new version of the application is pushed
          to all the servers. If instead it has a negative effect on business metrics, then all servers with the new
          version are rolled back.</p>
        <p>This approach allows an organization to run two versions of their website, collect user data on both
          versions, and compare the results. However, there are some trade-offs to this strategy.</p>
        <h4>3.1.1 Trade-off #1: Full redeploy to fix bugs</h4>
        <p>Using multiple deployments for serving two versions of a website requires a <strong>full take-down and redeploy to
          fix a bug</strong>. If there’s a critical bug in the new version of the application, the following steps must be
          completed:</p>
        <ol>
          <li>The rollout percentage must first be set to 0% so all requests will be handled by the group of machines
            with the old version of the application.</li>
          <li>The machines serving the new version must be rolled-back to handle traffic without crashing.</li>
          <li>The load balancer must be updated to allow rolled-back servers to receive requests again.</li>
          <li>Once the bug is fixed, the new version must be deployed, the rollout percentage must be set and the router
            must be configured accordingly.</li>
        </ol>
        <p>Many of these steps can be automated, but if the organization is not using a 3rd-party, they must spend
          resources to do that. For a small organization with minimal resources, this is undesirable.</p>
        <h4>3.1.2 Trade-off #2: Complexity when testing multiple features</h4>
        <p>As discussed earlier, if two versions of a feature are being served with multiple deployments means there are
          two versions of the entire application running in production. If another feature is also being tested at that
          time, that means another version of the application is needed to test it assuming that the two features being
          tested do not overlap.</p>
        <p>For example, if Solar Flair also wants to test out a new contact form that is hosted on a separate page from
          their quote form, then those two new features would not overlap. If instead they wanted to test changing the
          form’s submit button and wanted to see how each combination of the form and button affected user behavior,
          they’d need four total deployments.</p>
        <img src="./images/diagrams/3.1.2_multivariate_testing.png" alt="multivariate testing" width="500px">
        <p>Regardless of whether the additional new features being tested are overlapping or not, more tests results in:
        </p>
        <ul>
          <li>More deployed copies of an application.</li>
          <li>More versions of an application to keep track of, which can lead to human errors, such as implementing a
            change on the wrong version of the application.</li>
        </ul>
        <p>A large company with a large infrastructure and some DevOps engineers may not be be bothered by additional
          deployments of code and keeping track of the different versions of the application. But it would be more
          challenging for smaller organizations to implement this solution.</p>
        <br>
        <!-- Do a line break here if we can -->
        <p>Solar Flair only has a handful of developers and servers. Therefore it would not be wise to implement this
          strategy given the trade-offs. Fortunately for them, there’s a way to serve users two different versions of a
          site with just one instance of an application, through using <strong>feature flags</strong>.</p>
        <h3 id="section-3-2">3.2 Possible Solution: Feature Flags</h3>
        <p>“A feature flag is a software development process used to enable or disable functionality remotely without
          deploying code” [2].</p>
        <p><strong>A feature flag is conditional logic connected to a remote service that can alter its flow without a
          redeployment.</strong> One can think of a feature flag as a toggle that determines whether a feature is turned “on” or
          “off.” If the flag is turned “on”, the conditional returns <code>true</code> and the user receives one version of the
          application. If the flag is turned “off”, the conditional returns <code>false</code> and they receive another.</p>
        <img src="./images/diagrams/3.2_flag_evaluation.png" alt="Flag evaluation">
        <!-- Code snippet here -->
        <p>One can wrap as much or as little code in the feature flag as they like. It could be something big like a
          completely different UI, or something small, like different colors on a “Checkout” button.</p>
        <p>Feature flags can be <strong>static</strong>, meaning they’re either “on” or “off” for everyone; or <strong>dynamic</strong> meaning that
          they’re “on” or “off” for some users based on a criteria, like whether they’re in a “control group” or a “test
          group” of an A/B test. In order to make a flag dynamic, a “Toggle Router” is needed [3]. </p>
        <h4>3.2.1 Toggle Router</h4>
        <p>A toggle router is simply something that “can be used to dynamically control which codepath is live” [3]. The
          <code>evaluateFlag</code> function from the example above is a toggle router. One can implement toggle routers in many
          ways, like a simple in-memory store or a standalone app with a user interface (UI).</p>
        <p>A toggle router can be customized to evaluate a flag given any desired criteria. Using a toggle router, Solar
          Flair could set a rollout percentage so that a set percentage of users receive the new quote form while the
          rest receive the old one.</p>
        <p>For example, Jack and Jill are two potential customers, both looking for solar panel installations for their
          respective homes. They each visit <span id="fake-website">solarflair.net</span> in their browsers. A toggle router inputs their unique
          identifiers (like their IP address) into a hashing algorithm and determines that Jack will receive the old
          version of the quote form and Jill will receive the new version. All this happens using just one version of
          Solar Flair’s website.</p>
        <img src="./images/diagrams/3.2.1_jack_and_jill.png" alt="jack and jill comparison">
        <h4>3.2.2 - Using Feature Flags for A/B Testing</h4>
        <p>As mentioned earlier, two key challenges of implementing AB Testing are:</p>
        <p>1. Assigning users randomly into groups.</p>
        <p>2. Serving a user the same treatment on each visit.</p>
        <p><strong>Solar Flair can solve these two challenges by using feature flags with a toggle router.</strong></p>
        <h2 id="section-4">4. Existing Solutions</h2>
        <p>Solar Flair now knows they want to integrate feature flags into their web app and use them to perform A/B
          tests. What they don’t know is exactly how to implement this new system. Do they want to build it from scratch
          or integrate with an existing solution? If they want to build it, how much time and effort is needed to
          accomplish that?</p>
        <h3 id="section-4-1">4.1 DIY</h3>
        <p>If they decide to build the system, they’ll need to accomplish three things:</p>
        <ol>
          <li>Persistently store their feature flag data.</li>
          <li>Connect the flag data to their application to dynamically serve the two versions of their site.</li>
          <li>Collect user event data and analyze it to gain insights to drive their decision making.</li>
        </ol>
        <h4>4.1.1 Config File</h4>
        <p>The easiest and simplest way to store all the flag data in one place is by using a configuration file. This
          file would contain the flag data in a data structure and could look something like the code below:</p>
        <!-- Code snippet -->
        <p>The trade-offs with the config file are:</p>
        <ul>
          <li><strong>Flags can’t be set dynamically</strong>, they’re “on” or “off” for all users.</li>
          <li>Every time the config file is updated the application must be <strong>redeployed for the changes to take effect</strong>.</li>
        </ul>
        <p>The config file’s shortcomings make this option unappealing.</p>
        <h4>4.1.2 Database</h4>
        <img src="./images/diagrams/4.1.2_application_to_database.png" alt="applicaiton to database" width="300px">
        <img src="./images/diagrams/4.1.2_flag_in_database_table.png" alt="application to database table">
        <p>A slightly better option for storing flag data is by using a database. With this configuration, Solar Flair’s
          app would query the database each time a flag’s status is needed and updates to the flags would be made using
          SQL commands, which would be cumbersome for non-technical employees to manage feature flags. The main benefit
          of this architecture is it would allow flag data to be <strong>updated without redeployment</strong>. [4]</p>
        <p>However, it still doesn’t solve the core problem of dynamically evaluating the flags. In addition, <strong>neither of
          these two options address the analysis of the data</strong>. What Solar Flair needs is something bigger and more
          robust. A place to easily manage their flags and their experiments. What they need is a <strong>feature flag
          management service</strong>, specifically one that can handle A/B testing.</p>
        <h4>4.1.3 Building a Feature Flag Management Service</h4>
        <p>Feature flag management services are platforms that allow developers to view all their feature flags in one
          place, usually through a user interface. They make the process of performing CRUD operations on the flags
          easier to implement and keep track of. In addition, the services often contain tools, such as custom flag
          evaluations, allowing developers to get more out of their feature flags.</p>
        <p>To build this system oneself, one will need to build both the feature flagging and A/B testing services. One
          must first design and build the architecture to manage and evaluate feature flags. This will require several
          components such as a frontend, backend, database, and SDK at a minimum. </p>
        <ol>
          <li>A <strong>frontend interface</strong> will enable both developers and non-technical members of the organization to
            easily manage feature flags and experiments.</li>
          <li>A <strong>backend</strong> server and database will enable persistent storage of data.</li>
          <li>An <strong>SDK</strong> that lives in the organization’s app should return the “on” or “off” status for a given user
            based on factors like rollout percentage. It is typically necessary to have SDKs on both the client and
            server, since there are features to be managed on both.</li>
          <li>The SDKs also need to receive <strong>updated feature flag data</strong> when a change is made from the frontend,
            ideally in real-time so that features can be shut off immediately if needed.</li>
        </ol>
        <img src="./images/diagrams/4.1.3_diy_architecture.png" alt="diy architecture">
        <h4>4.1.4 Building the A/B Testing Functionality</h4>
        <p>In addition to the infrastructure for feature flag management, one must also build the A/B testing
          functionality to gain insights from changes in user event data. There are 4 steps to this:</p>
        <ol>
          <li>Collect the user event data.</li>
          <li>Process the data.</li>
          <li>Run statistical analysis.</li>
          <li>Display the results.</li>
        </ol>
        <p>For the first step, a system must be in place to <strong>log user events</strong>  and save them to a <strong>database</strong> using ETL
          pipelines. This infrastructure can be built in-house, or a 3rd party service like Mixpanel can be used.
          However, using a 3rd party service will offer less flexibility.</p>
        <p>Then, the data must be processed by <strong>querying the event data</strong> and <strong>calculating metrics</strong> for each experiment and
          treatment.</p>
        <p>Next, the metrics are used as inputs in <strong>statistical tests</strong> to determine whether the change in them is likely
          due to the change in feature or to random chance.</p>
        <p>Lastly, the results are displayed, ideally with some <strong>visualization</strong>.</p>
        <p>Having a data scientist on a team to give guidance would make the last three steps go smoothly. For example,
          a data scientist would know which statistical tests to run depending on the data types, how the metrics need
          to be measured and aggregated, and which kinds of visuals are most helpful. Without someone with that
          knowledge, a team may have trouble building an A/B testing solution.</p>
        <h4>4.1.5 Trade-offs</h4>
        <p>Building your own platform for feature flagging and experimentation comes with several benefits:</p>
        <ul>
          <li><strong>Flexibility over how it is deployed</strong>, and ability to customize what features to include and how they are
            implemented. For example, the team could choose whether to use classical statistics or Bayesian statistics for
            the experiment analysis.</li>
          <li><strong>Keep data in-house</strong>, reducing concerns around privacy and security</li>
        </ul>
        <p>However, this approach also comes with a set of trade-offs:</p>
        <ul>
          <li><strong>Time and resources</strong> to build the feature flag management and A/B testing platform</li>
          <li>Responsibility to <strong>maintain</strong> the system into the future</li>
        </ul>
        <p>As we saw earlier, building an A/B testing platform completely from scratch is extremely hard and time
          consuming (expect at least 2,000 hours of work to get something decent) [5]. In other words, building the
          platform yourself is not free. </p>
        <p>One must also consider the <strong>accuracy and reliability</strong> of the system. Maintaining reliable analytics pipelines
          is more difficult than one may think [6]. Therefore, as the ones who created the system, the team will also be
          responsible for maintaining it into the future.</p>
        <h3 id="section-4-2">4.2 Paid Solutions</h3>
        <p>The other option is to use a 3rd party feature flagging and A/B testing platform that handles the engineering
          aspects for you so that your team does not have to invest time into building and maintaining this system. Some
          well-known players are LaunchDarkly and Optimizely. Both are feature-rich and reliable. LaunchDarkly is a
          popular feature flag management platform that offers a wide range of features, such as A/B testing, targeting
          users by attribute, integrations with productivity tools, workflow automation, and a support team. Optimizely
          is a similar platform which focuses primarily on A/B testing. However, there are several trade-offs that come
          with this approach:</p>
        <ul>
          <li>Since they are built by an external team, there is not as much flexibility for <strong>customization</strong> as an in-house
            platform. They would not be able to change what features to include and their implementation, or the types of
            statistical tests to use.</li>
          <li>Using one of these services entails letting a third party <strong>store</strong> user data, which some business may not be
            comfortable with or legally allowed to.</li>
          <li>They typically <strong>cost</strong> a monthly fee and can be out of budget for smaller companies. </li>
        </ul>
        <h2 id="section-5">5. Waypost</h2>
        <p>If Solar Flair was a bigger company with more engineers, then building their own system could be a good
          option. If they wanted a large set of features quickly and had a larger budget, then a paid solution like
          LaunchDarkly would be a good choice. However, the DIY option costs too much time and resources, and the paid
          option is too expensive for them and doesn’t allow them to customize the functionality. They also don’t need
          many extra features, as they only need to run simple experiments.</p>
        <p>Therefore, there was an opportunity to create a feature flagging platform that fits the needs of small
          businesses like Solar Flair. We created Waypost, a feature flag management platform that specializes in A/B
          Testing. <strong>Waypost is self-hosted and open-source, so it is completely customizable while still containing the
          core features our target user needs.</strong></p>
        <img src="./images/diagrams/5_competitor_comparison.png" alt="competitor comparison">
        <h3 id="section-5-1">5.1 Architecture Overview</h3>
        <p>Waypost provides a feature flag and A/B testing solution through the integration of it’s own components
          (colored blue) and some existing infrastructure (colored red). The existing infrastructure is the application
          that is using feature flags and a PostgreSQL database for storing user event data, which will be referred to
          as the “Events DB”. For example, Solar Flair’s “Application” would be their web application that renders their
          website and their “Events DB” would be their existing user event database.</p>
        <img src="./images/diagrams/5.1_waypost_architecture.png" alt="Waypost architecture" width="800px">
        <p><span class="underline">General Overview of the Components and their responsibilities:</span></p>
        <ul>
          <li>The <strong>Manager Application</strong> is responsible for managing feature flags and experiments.</li>
          <li>The <strong>Manager App</strong> sends copies of the flag data to the <strong>Flag Provider</strong>, which saves the copy and forwards it to
            each <strong>application</strong> running the <strong>Waypost SDK</strong>.</li>
          <li>The <strong>SDK</strong> is embedded into an <strong>application</strong> and is responsible for evaluating flags at run-time, and thus
            allowing one to serve multiple versions of their website to their users. It also performs the assignment of
            users into treatments.</li>
          <li>The developer using Waypost is responsible for supplying their existing user event logging solution, in which
            their <strong>application</strong> sends user event data to their <strong>Events DB</strong>.</li>
          <li>The <strong>Manager App</strong> queries the <strong>Events DB</strong> when it runs statistical analysis for the experiments and displays the
            results in the <strong>Manager’s UI</strong>.</li>
        </ul>
        <h3 id="section-5-2">5.2 Manager Application</h3>
        <img src="./images/diagrams/5.2_Manager_app.png" alt="Manager app">
        <p>The <strong>Manager App</strong> is what developers use to manage their feature flags and view experiment results. It also
          contains a <strong>statistics pipeline</strong> that performs the querying of data and statistical analysis for experiments.
          The Manager App has three components, a React.js application that serves as the User Interface (UI), a backend
          server built with Express.js, and a PostgreSQL database used for data persistence. </p>
        <p>The Manager App’s <strong>UI</strong> provides features for developers and non-technical members of an organization to manage
          their flags and experiments:</p>
        <h4>Flags Dashboard</h4>
        <p>The Flags Dashboard displays all the flags being used by an application. From here you can toggle, delete,
          and create flags.</p>
        <img src="./images/diagrams/5.2_flags_dashboard.gif" alt="flags Dashboard demo" width="800px">
        <h4>Flag Details Page</h4>
        <p>To visit the Details Page for any flag, click on a flag’s name on the Flags Dashboard. On this page can view
          and edit the <strong>name</strong>, <strong>description</strong>, and <strong>rollout percentage</strong> of the flag.</p>
        <img src="./images/diagrams/5.2_flags_detail_page.gif" alt="flag details page" width="800px">
        <p>This page also allows one to view and edit <strong>custom assignments</strong>. Custom assignments are used to ensure that a
          flag evaluates a certain way for a specific user. </p>
        <p>For example, an internal member testing Solar Flair’s new quote form would add a custom assignment so when
          they connect to Solar Flair’s website the SDK will always serve them the new version of the quote form.</p>
        <img src="./images/diagrams/5.2_custom_assignments.gif" alt="Custom assignment form" width="800px">
        <h4>Create Experiment Form</h4>
        <p>From a Flag’s Details Page, one can create an experiment on a flag. The most important inputs are the
          <strong>percentage of users tested</strong> and the <strong>metrics</strong> being tested. Note, Waypost doesn’t stop experiments automatically,
          the <strong>duration</strong> input is only to help the experimenter plan.</p>
        <img src="./images/diagrams/5.2_create_experiment_form.gif" alt="Create experiment form" width="800px">
        <h4>Flag Events Log</h4>
        <p>The Flag Events Log is used to keeping track of the changing state of one’s flags. It displays all <strong>flag
          events</strong>, like creation, deletion, toggling, and editing of a flag. This page is most useful for debugging,
          helping a developer make connections between bugs reported in their application and feature flags.</p>
        <img src="./images/diagrams/5.2_event_log.gif" alt="Event Log" width="800px">
        <h4>SDK Key</h4>
        <p>This page is used to create an SDK key, which is used in SDK to Flag Provider communication.</p>
        <img src="./images/diagrams/5.2_SDK_key.jpg" alt="SDK Key" width="700px">
        <p>The Manager App’s <strong>Backend Server</strong> is responsible for:</p>
        <ul>
          <li><strong>Managing API endpoints</strong> for supplying and editing flag and experiment data.</li>
          <li><strong>Sending flag data</strong> and the SDK key to the Flag Provider.</li>
          <li>The <strong>statistics pipeline</strong>, which involves querying the Events DB for metric data at the end of an experiment
            and performing statistical analysis on it to obtain the experiment’s results.</li>
          <li>The <strong>exposures pipeline</strong>, which involves querying the Events DB once a day for exposures data and aggregating
            it. The exposure data can be used by developers to know when to stop an experiment.</li>
        </ul>
        <img src="./images/diagrams/5.2_manager_app_responsibilities.png" alt="Manager App Responsibilities">
        <p>The Manager App’s PostgreSQL <strong>Database</strong> stores persistent experiment and flag data. Some examples are:</p>
        <ul>
          <li><strong>Data pertaining to a flag</strong>, like it’s name, status (on/off), and rollout percentage.</li>
          <li><strong>Flag event data</strong>, like at what time a flag was toggled, created, edited, or deleted.</li>
          <li><strong>Custom assignment data</strong>, which is used to evaluate a flag a desired way for specific users.</li>
          <li><strong>High-level experiment data</strong>, like id of the flag an experiment is on, the metrics being measured, the amount
            of users placed in the test and control groups, and the results of the experiment.</li>
          <li>The most recent <strong>SDK key</strong>.</li>
          <li><strong>Database connection data</strong>, such as the credentials and URL of the Events DB.</li>
          <li><strong>Query strings</strong> used by the Manager App’s Backend to query the Events DB.</li>
        </ul>
        <h3 id="section-5-3">5.3 Flag Provider Service</h3>
        <p>The Flag Provider is a lightweight service that acts like a cache for flag data. It is responsible for
          providing up-to-date flag data to instances of the SDK.</p>
        <img src="./images/diagrams/5.3_flag_provider.png" alt="Flag Provider">
        <h4>5.3.1 Why make providing flags a separate service?</h4>
        <p>As mentioned previously, the SDK is responsible for evaluating flags, so it must have a copy of that flag
          data and that flag data must be updated in real time in case a feature needs to be turned off immediately. The
          Manager App, which stores the flag data, could be connected to the SDK directly to serve it flags. </p>
        <p>However, this solution isn’t very scalable because the specific functionality of managing SDK connections is
          intertwined with the Manager Application’s other duties. <strong>Scalability</strong> is important in this case, because the
          number of applications running the SDK can fluctuate significantly. </p>
        <p>A better architecture for Waypost’s use case includes the Flag Provider, a service that is only responsible
          for sending updates to the SDK in real-time. <strong>The Flag Provider is lightweight and thus easily scalable.</strong></p>
        <h4>5.3.2 Flag Provider Scaling</h4>
        <p>The Flag Provider Service has been load tested to determine how many connections to SDK clients it can
          handle. The maximum amount of connections is important for knowing when one would need to horizontally scale
          the Flag Provider. Based on our environment, response times were extremely fast. However, connections were
          refused starting at around 318 requests per second, indicating that the <strong>Flag Provider could sufficiently
          handle around 300 SSE connections at once</strong>. An organization using Waypost would therefore need to deploy more
          instances of the Flag Provider based on how many concurrent users they may have.</p>
        <h3 id="section-5-4">5.4 SDK</h3>
        <p>Waypost provides SDK’s for <strong>React.js</strong> (client-side) and <strong>Node.js</strong> (server-side). The SDK stores copies of the
          flag data and is responsible for <strong>evaluating flags</strong>, thus it is also responsible for rendering two versions of a
          site. In addition, the SDK assigns users into <strong>treatment groups</strong> for experiments.</p>
        <img src="./images/diagrams/5.4_SDK.png" alt="Waypost SDK">
        <p>The developer can use the SDK client’s <code>evaluateFlag(flagName)</code> method which will return a boolean value. This
          returned value is used in the app’s code to deliver one feature or another.</p>
        <!-- Code Snippet -->
        <h4>5.4.1 How Feature Evaluation Works</h4>
        <p>There are three flag attributes the <code>evaluateFlag</code> takes into account when evaluating:</p>
        <ol>
          <li>The <strong>status</strong> of a feature flag (on or off).</li>
          <li><strong>Custom assignments</strong> on the flag.</li>
          <li><strong>Rollout percentage</strong> of the flag.</li>
        </ol>
        <img src="./images/diagrams/5.4.1_flag_evaluation_flow_chart.png" alt="Flag evaluation flow chart">
        <p>The custom assignment and rollout evaluation takes an additional input, a <strong>unique identifier</strong> (often a user
          ID). For custom assignments, the unique identifier associated with the user of the application is compared to
          the existing custom assignments on the flag.</p>
        <p>For evaluating flags with rollout percentages (which includes all flags with experiments on them), a <strong>hashing
          function</strong> takes the user’s unique identifier as input and outputs a number from 0 to 99. Then, if the hash is
          less than the rollout percentage, the flag is “on” for that user, otherwise it is “off”.</p>
        <p>The above explanation is the basis of how flags are evaluated with rollout percentages, but there’s a
          downside. If an application has multiple flags with experiments, some users would be placed in the test group
          for the majority of experiments instead the placement being more random. This is problematic because it
          increases the likelihood the one experiment would affect another. It introduces an external variable.</p>
        <p>That’s why Waypost uses an <strong>offset</strong> that’s assigned to each flag with a rollout percentage. The offset moves
          the range of hashed user IDs that a flag is “on” for. For example, without an offset, two flags (Flag #1 and
          Flag #2) with 50% rollout percentages would both be “on” for users with hashed IDs from 0-49. </p>
        <img src="./images/diagrams/5.4.1_flag_rollout_1.png" alt="flag rollout 1" width="400px">
        <p>If instead Flag #1 is given an offset of 30 and Flag #2 is given an offset of 5, then hashed IDs from 30-79
          would receive Flag #1 and 5-54 would receive Flag #2.</p>
        <img src="./images/diagrams/5.4.1_flag_rollout_1.png" alt="flag rollout 1" width="400px">
        <h3 id="section-5-5">5.5 How Flag Data is Sent From Manager To SDKs</h3>
        <p>The SDK receives feature flag data from the Flag Provider Service via <strong>SSE</strong> (Server-Sent Events). </p>
        <p>When an application that contains the Waypost SDK starts running, it sends an initial GET request to the Flag
          Provider to fetch the current feature flag data.</p>
        <p>The <strong>SDK key</strong> is attached to this request and is validated by the Flag Provider before sending back a response.
          If the SDK key is invalid, the data will not be returned. </p>
        <p>Whenever a change is made to a feature flag, the Manager App sends a <strong>webhook</strong> to the Flag Provider. This then
          triggers the Flag Provider to send the updated set of feature flags to the SDK via SSE.</p>
        <img src="./images/diagrams/5.5_data_flow.gif" alt="Waypost data flow">
        <h3 id="section-5-6">5.6 How A/B Testing Works with Waypost</h3>
        <p>A/B Testing with Waypost involves three components, the Application (with the Waypost SDK within it), the
          Events DB, and the Manager App.</p>
        <ul>
          <li>The Application places users in <strong>treatments</strong> and sends <strong>user event data</strong> to the Events DB.</li>
          <li>Developers manage experiments in the Manager App’s UI. This includes creating <strong>metrics</strong> for their experiments,
            <strong>creating and stopping experiments</strong>, and viewing the results.</li>
          <li>The Manager App’s Backend queries the Events DB for <strong>exposures</strong> and <strong>metric</strong> data and performs <strong>statistical
            analysis</strong>, and stores the results of the analysis in its own database.</li>
        </ul>
        <img src="./images/diagrams/5.6_user_event_diagram.png" alt="User event data diagram">
        <h4>5.6.1 Events DB</h4>
        <p>In order to perform an A/B test with Waypost, the developers must have a database (that we call the Events
          DB) set up. The Events DB must contain their app’s user event data (e.g., clicks, impressions, conversions).
          This database must also contain a table that stores which treatment each user is assigned to when they are
          exposed to an experiment.</p>
        <h4>5.6.2 Connecting Manager App to Events DB</h4>
        <p>Before starting an experiment, the developer must connect the Events DB to the Manager App. They also must
          create the metrics they want to measure in the experiment.</p>
        <p>To connect the Events DB and Manager App:</p>
        <ol>
          <li>The Events DB must give read access to the Manager App</li>
          <li>The developer must input the Events DB credentials and a query string into the database connection form in
            the UI (shown above). The query string is used to query the Events DB for exposures and must return data in
            the schema shown below.</li>
        </ol>
        <img src="./images/diagrams/5.6.2_experiments_table.jpg" alt="Experiments table" width="600px">
        <img src="./images/diagrams/5.6.1_database_connection_form.gif" alt="database connection form" width="800px">
        <h4>5.6.3 Metrics</h4>
        <p>To add metrics, the developer must provide:</p>
        <ul>
          <li>The <strong>name</strong></li>
          <li>The <strong>type</strong> of metric</li>
          <li>A <strong>query string</strong>, which is used to query the Events DB</li>
        </ul>
        <img src="./images/diagrams/5.6.3_new_metric_page.gif" alt="New metrics page" width="800px">
        <p>After the Manager App confirms it can query the Events DB to obtain a metric, the developer can attach that
          metric to an experiment. Some common examples of metrics are:</p>
        <ul>
          <li>Click-through rate</li>
          <li>Signup rate</li>
          <li>Time spent on site</li>
        </ul>
        <h4>5.6.4 Experiment Exposures Pipeline</h4>
        <img src="./images/diagrams/5.6.4_exposures_pipeline.png" alt="Exposures pipeline">
        <p>Once an experiment has been created, Waypost aggregates and analyzes this data. The Experiment Exposures
          Pipeline runs once every night to aggregate the number of users bucketed into each cohort of the experiment,
          and stores the aggregated data into a table in the Manager App’s database. In case the pipeline ever fails, it
          also checks for and backfills any missing data each time it runs. The Manager App then displays this as a line
          chart essentially depicting the sample size of each cohort over time. <strong>The purpose of this visualization is to
          help the developer understand the sample size and determine how much longer the experiment should run.</strong></p>
        <img src="./images/diagrams/5.6.4_exposures_graph.jpg" alt="Exposures Graph">
        <h4>5.6.5 Statistics Pipeline</h4>
        <p>Once the developer is ready to view the results of their experiment, they can click on the “Refresh Results”
          button in the Manager App’s UI. This triggers the Statistics Pipeline, which:</p>
        <ol>
          <li><strong>Queries</strong> the Events DB for the metrics being tracked for this experiment.</li>
          <li><strong>Analyzes</strong> this data using statistical tests, using the *t-test* for continuous metrics (count,
            duration, revenue) and the *chi-squared* test for discrete metrics (binomial).</li>
          <li><strong>Stores</strong> the results in the Waypost DB, which will be displayed in the UI as a table (shown below).</li>
        </ol>
        <p>Information on how to interpret the statistical results are included in our <a href="https://waypost-io.github.io/documentation/">documentation site</a>.</p>
        <img src="./images/diagrams/5.6.5_results_page.jpg" alt="Results page" width="1000px">
        <h2 id="section-6">6. Engineering Decisions and Trade-Offs</h2>
        <h3 id="section-6-1">6.1 Hosted vs. Self-Hosted</h3>
        <p>To fully implement Waypost, a number of important engineering decisions had to made, one of which was whether
          the Waypost application would be hosted on the cloud or hosted by our users. We opted for a self-hosted option
          for a couple reasons:</p>
        <h4>6.1.1 Flexibility in Deployment Options</h4>
        <p>With a self-hosted product, users have the freedom to <strong>deploy Waypost in whichever environment they prefer</strong>,
          whether it’s deploying to a cloud service or hosting it on their local infrastructure. This means users can
          choose how the Waypost application is monitored and scaled.</p>
        <h4>6.1.2 Data Security and Availability</h4>
        <p>Having a feature flag management service hosted on-site keeps security concerns to a minimum by reducing an
          organization’s reliance on third-parties to host data. It’s impossible to know for sure that an external host
          will keep their data secure and that their servers will always be available. A self-hosted solution keeps
          these concerns in-house with full control of <strong>security and availability</strong>.</p>
        <h3 id="section-6-2">6.2 Collecting User Event Data</h3>
        <p>A critical component of A/B testing is the <strong>user event data</strong> that comes from the A and B versions of the
          application. Without it, no analysis can be performed. To collect user event data for analysis, there were to
          options to choose from:</p>
        <ol>
          <li>Let the <strong>organization collect the data</strong> and have Waypost query their database.</li>
          <li>Have <strong>Waypost collect the data</strong> and store it in its own database.</li>
        </ol>
        <img src="./images/diagrams/6.2_collecting_user_event_data.png" alt="Collecting user event data" width="1000px">
        <p>To make the decision, we considered our target user.</p>
        <p>Like most companies interested in A/B testing, Solar Flair already has existing infrastructure to collect and
          store user event data. If Waypost was designed to collect user event data in order to run its statistical
          analysis, then an organization would need to add additional logging functions into their code to accommodate
          the second analytics platform. <strong>Waypost’s data collection system would be redundant</strong>.</p>
        <img src="./images/diagrams/6.2_collecting_user_event_data_2.png" alt="Collecting user event data 2" width="500px">
        <p>With this in mind, we thought it would be advantageous to use the existing data instead of duplicating it by
          hosting our own analytics platform. By opting out of deploying an analytics platform with Waypost,
          organizations can remain in full control over their own event logging processes.</p>
        <h3 id="section-6-3">6.3 Communication Between Manager App and Flag Provider</h3>
        <p>In the earliest implementation of Waypost, the SDK clients connected directly to the Manager App. However,
          there are a number of issues with this approach.</p>
        <h4>6.3.1 The Downsides of the SDK and Manager App Communicating Directly</h4>
        <p>The biggest challenge is that the number of clients connecting to the Manager App is potentially quite large
          since there would be one instance for each visitor to the site. For example, if there were 500 users of an
          application with our SDK in it, the Manager App would have to manage 500 SSE connections in addition to
          handling the UI’s API calls and running statistical analysis. The sheer volume of requests creates a
          <strong>bottleneck</strong> that could have adverse effects on either the clients or the users interacting with the Manager
          App.</p>
        <img src="./images/diagrams/6.3.1_backend_server_connected_to_SDK.png" alt="Backend connected to SDK">
        <p>Another consequence of this architecture is that <strong>clients interface directly with the server that hosts
          Waypost’s data</strong>. If a bad actor knew the IP address of the Manager App, it would be susceptible to attacks. The
          alternative of separating these concerns by only allowing clients to interact with the Flag Provider minimizes
          exposure to Waypost’s data, thus creating a layer of security.</p>
        <p>Lastly, coupling the management and serving of flag data means that the <strong>two services must scale up together</strong>
          instead of separately. With a separate Flag Provider, a developer can start up a new instance if the volume of
          SDK clients is too large for just a single instance to handle. This also allows an organization to reduce
          latency to the SDK clients by creating new flag provider instances physically closer to their users.</p>
        <h4>6.3.2 How should the Flag Provider and Manager App Communicate?</h4>
        <p>Due to these concerns, we decided to implement an intermediary service to provide feature flag data to the
          SDK clients. To create such a service, we need a way for the Flag Provider to receive updated flag data from
          the Manager App. The three major options we considered implementing were polling, a message broker, and
          webhooks.</p>
        <p><strong>Polling</strong> is a method of communication that uses HTTP requests sent at a predefined interval to receive updated
          data. For example, if the Flag Provider used polling, it would send a GET request to the Manager App every 10
          seconds. As a response the Manager App would send the flag data. The downsides to this method are the lack of
          real-time updates as requests are sent every 10 seconds. Also, this method means that lots of unnecessary
          requests are getting sent between servers, because the there will be intervals when the flag data never
          changes. If the flag data doesn’t change then the Flag Provider is sending GET requests for data that’s
          already up to date.</p>
        <img src="./images/diagrams/6.3.2_polling.png" alt="Polling">
        <p>To remedy the issues with polling, it’s possible to use a similar method known as “long polling”. With <strong>long
          polling</strong>, a request gets sent to the server, and a response is only sent back when the server has a message to
          send. In the context of Waypost, an SDK client would send a request to the Flag Provider. If the flag data has
          changed since the previous GET request, then the server would send back the updated flags. Otherwise, the
          server would wait to send a response until the flags are updated. Despite its benefits, we decided against
          long polling because it still requires a high volume of requests to be verified and processed by the server.
        </p>
        <img src="./images/diagrams/6.3.2_long_polling.png" alt="Long-polling">
        <p>Another option we considered was a <strong>message broker</strong>. A message broker is a commonly used tool that allows
          services to communicate with one another. A message broker exchanges information through a system of
          publishers and subscribers. Within the context of Waypost, here’s how a message broker would work:</p>
        <ol>
          <li>In the event that the flag data is updated, a message containing the flag data is published to the message
            broker.</li>
          <li>The Flag Provider pulls the message from the message broker and saves the flag data to memory.</li>
        </ol>
        <p>A message broker is a good option because communication between the two services will be event-driven and
          happen in real-time. Also, the message broker gives some degree of <strong>fault tolerance</strong>. If the Flag Provider goes
          down for any reason, messages sent from the Manager App will be available once it is up and running again.
          Additionally, a message broker eliminates the need to create API endpoints on the two services.</p>
        <p>The main trade-offs to this approach are that it would increase the <strong>complexity</strong> of the architecture. There’s
          now another component to maintain and secure. Message brokers can also make debugging difficult because
          there’s now another component logging errors.</p>
        <p>Due to these reasons, we turned to another option, webhooks.</p>
        <img src="./images/diagrams/6.3.2_message_broker.png" alt="Message Broker">
        <p>A <strong>webhook</strong> is an HTTP request that gets sent to another server when a predefined event occurs. In our case,
          whenever an update is made to either the feature flag data or the SDK key, the Manager App sends a webhook to
          the Flag Provider with the updated data. The Flag Provider saves the data to memory and forwards the flag data
          to any SDK clients that are connected. Using this <strong>event-driven</strong> approach of sending a webhook on updates allows
          the Flag Provider to have the latest version of the flag data.</p>
        <img src="./images/diagrams/6.3.2_webhook.png" alt="Webhook">
        <p>The Flag Provider also should only make updates to its flag data if the webhook it received was from the
          Manager App. To implement this, the Manager App attaches a header to each webhook with a secret key that only
          the Flag Provider and Manager App have access to. If the verification header does not match the secret key,
          then the request is rejected.</p>
        <h3 id="section-6-4">6.4 Providing Feature Flag Data to SDK</h3>
        <p>The main purpose of the Flag Provider is to ensure that clients running the Waypost SDK receive up-to-date
          feature flag data. This ensures that SDK clients will have the data they need when they’re expected to have
          it. One example of why this is crucial is in event of a new feature causing an application-crashing bug. A
          developer, when notified of the issue, would toggle the flag that contained the new feature. One would want
          that new flag data to be sent to the SDKs as fast as possible upon toggling. </p>
        <p>The options we considered for communicating between SDK clients and the Flag Provider were WebSockets and
          Server-Sent events (SSE).</p>
        <h4>6.4.1 WebSockets vs. Server-Sent Events</h4>
        <p><strong>WebSocket</strong> is a communication protocol that allows <strong>bidirectional communication</strong> through a single TCP
          connection. The WebSocket protocol can be described using three phases:</p>
        <p>1. An initial GET request with some special headers are sent from the client to the server. This allows the
          server to establish a connection using the WebSocket protocol.</p>
        <p>2. The client and server communicate with each other. Messages can be sent back and forth using the WebSocket
          connection.</p>
        <p>3. The connection is closed when either the client and server call a “close connection” method.</p>
        <p>WebSocket is useful because only one connection needs to be made and communication between the two entities
          is real time. The main downside of WebSockets, for our use case, is that we don’t ever need to send messages
          from the client to the server. In other words, we don’t need two-way communication. Another minor concern of
          using WebSocket is that it adds some development overhead. It typically requires more code than other methods,
          and it bars us from using useful HTTP functionality.</p>
        <img src="./images/diagrams/6.4.1_websocket.png" alt="WebSocket" width="400px">
        <p>Server-Sent Events, or <strong>SSE</strong>, only allows for <strong>unidirectional communication</strong> between the server and a client.
          Whenever feature flag data is updated on the Flag Provider, new data is sent to all the clients that are
          connected. This approach minimizes latency, creates a real-time stream of feature flag data, and reduces the
          number of connections that need to be made with the server. Here’s how SSE works:</p>
        <ol>
          <li>The client sends a request to the server.</li>
          <li>The server responds, confirming that the connection has been made.</li>
          <li>The server can now send messages to the client in response to events.</li>
        </ol>
        <p>The drawback to using SSE is that the connection can occasionally be dropped. Connections are dropped when a
          browser exceeds six open connections or the connection remains inactive for 30 seconds. For this reason, the
          Waypost SDK automatically re-establishes lost connections after a connection is lost using the EventSource
          API.</p>
        <img src="./images/diagrams/6.4.1_SSE.png" alt="SSE">
        <p>To ensure that each SSE connection is valid, the SDK client sends an <strong>SDK key</strong> in the initial request to the
          Flag Provider. The Flag Provider, who stores a copy of the SDK key, checks that the stored key matches the key
          provided by the client. Invalid keys are rejected, while valid ones are permitted to make a connection.</p>
        <h3 id="section-6-5">6.5 Statistics Pipeline</h3>
        <p>The statistics pipeline is a crucial piece of our architecture. The pipeline consists of three steps:</p>
        <ol>
          <li>Aggregate the metrics from the Events DB during the experiment’s time frame.</li>
          <li>Group the data by experiment and by treatment group.</li>
          <li>Run statistical tests on the data and write results.</li>
        </ol>
        <p>This pipeline could take a lot of time and resources to run if a company had a large volume of user event
          data and several experiments running. Thus, it was important to decide:</p>
        <ul>
        <li><strong>How often to run</strong> this pipeline.</li>
        <li>Whether to deploy on a <strong>separate server</strong> or on the <strong>Manager App’s server</strong>. If the pipeline took a long
          time to run, then having it on the Manager App’s back-end could block other operations such as updates to
          flags.</li>
        </ul>
        <img src="./images/diagrams/6.5_seperate_stats_server.png" alt="seperate stats server">
        <h4>6.5.1 How often to run the pipeline</h4>
        <p>If the pipeline was run frequently, such as each time the page was loaded, the developer using Waypost could
          quickly access up-to-date experiment results without having to wait, which would be the ideal user experience.
          However, the trade-off with this is that it would take up a large amount of computing resources. </p>
        <p>This can be optimized by taking into account how often a developer would need to see the results. As a
          general rule, experimenters should not look at an experiment’s results until it is completed, because peeking
          increases the false positive rate [7]. That means there isn’t a need to constantly refresh results.</p>
        <p>Therefore, Waypost has a button called “Refresh Results”, which triggers the statistics pipeline when
          clicked. This means that the <strong>results will only be computed when necessary, which saves server resources</strong>.</p>
        <h4>6.5.2 Deploy pipeline on separate server or on Manager App’s server</h4>
        <p>A concern with this approach was that the pipeline may take a long time to run, <strong>blocking other operations</strong> and
          leading to a negative experience for the user. However, the pipeline took less than a second to run when
          tested with a dataset of 10,000 users in the experiment with 2 metrics (our estimate for the average Waypost
          user), thus eliminating that concern. Given the performance, Waypost has the statistics pipeline on the same
          server as the Manager App instead of provisioning a dedicated server for it. <strong>The pipeline runs infrequently
          and typically takes less than a second to run, therefore one does not have to worry about it blocking other
          operations.</strong></p>
        <h2 id="section-7">7. Future Work</h2>
        <h3 id="section-7-1">7.1 Extended Database Integration</h3>
        <p>As Waypost currently exists, developers can only connect to an external <strong>PostgreSQL</strong> database when analyzing
          user event data. In reality, organizations use a variety of <strong>different database technologies</strong>. Companies who
          store user event data with something besides PostgreSQL will need to either load the data into a Postgres
          database themselves, or alter Waypost’s source code to be able to connect to something different. A future
          iteration of Waypost would aim to address this need by adding more database support.</p>
        <h3 id="section-7-2">7.2 Feature Flags by Application</h3>
        <p>Another valuable addition to Waypost would be to separate feature flag data by application. If an
          organization runs multiple applications, it would be ideal for each application to only receive feature flag
          data that pertained to that it. Currently, Waypost sends each application all the feature flag data.
          Separating feature flags by application would <strong>reduce the amount of data being sent to each SDK client</strong> and also
          make it so applications would only be able to read their <strong>own</strong> feature flag data.</p>
        <h3 id="section-7-3">7.3 Login Capability</h3>
        <p>A login capability would have many useful benefits, especially in terms of administration. By requiring users
          to log in to the management application, an admin would be able to keep track of and control <strong>who edits data
          and who has permission</strong> to edit data. To adhere to the principle of least privilege, an admin could make sure
          that each team <strong>only has access</strong> to the exact data they need. Also, if a flag looks like it was toggled
          erroneously, an admin could check who edited it last and reach out to them to discuss whether it was a mistake
          or not.</p>
        <h3 id="section-7-4">7.4 Additional Language Support for SDKs</h3>
        <p>To allow Waypost to be used by as many developers as possible, it would be beneficial to add additional
          language support for SDKs. As of right now, Waypost can only support users who use React and/or node for their
          applications. Waypost could expand its usage by adding support for other common languages such as Go, Ruby,
          Python, Java, and C++.</p>
        <h2 id="section-8">8. Glossary</h2>
        <ul>
          <li>
            <strong>Metrics</strong> - measures of quantitative assessment commonly used for assessing, comparing, and tracking
            performance or production
          </li>
          <li>
            <strong>Controlled Experiment</strong> - an experiment in which all the variable factors in an experimental group and
            a comparison control group are kept the same except for one variable factor in the experimental group that
            is changed or altered
          </li>
          <li>
            <strong>Statistically Significant</strong> - a determination that a relationship between two or more variables is
            caused by something other than chance.
          </li>
          <li>
            <strong>Data Pipeline</strong> - a set of actions that ingest raw data from disparate sources and move the data to a
            destination for storage and analysis <a
              href="https://www.stitchdata.com/resources/what-is-data-pipeline">[source]</a>.
          </li>
          <li>
            <strong>Backfill</strong> - any process that involves modifying or adding new data to existing records in a dataset
          </li>
          <li>
            <strong>Continuous data</strong> - data that can take any value within a range, such as a person’s weight
          </li>
          <li>
            <strong>Discrete data</strong> - data that is countable and distinct, and can only take certain values, such as number
            of signups.
          </li>
        </ul>
        <h2 id="section-9">9. References</h2>
        <ol>
          <li>
            Gallo, Amy. “A Refresher on A/B Testing”, https://hbr.org/2017/06/a-refresher-on-ab-testing
          </li>
          <li>
            Parzych, Dawn. “What are Feature Flags”, https://launchdarkly.com/blog/what-are-feature-flags
          </li>
          <li>
            Hodgson, Pete. “Feature Toggles (aka Feature Flags)”, https://martinfowler.com/articles/feature-toggles.html
          </li>
          <li>
            Haddad, Rowan. “Fun with Flags: Where to Store Feature Flags”, https://www.flagship.io/feature-flag-storage
          </li>
          <li>
            Dorn, Jeremy. “Open Source A/B Testing”, https://medium.com/growth-book/open-source-a-b-testing-dbc68aedab70
          </li>
          <li>
            Pam, Robin. “Build vs. Buy: Choosing the Right Experimentation Solution”,
            https://www.optimizely.com/insights/blog/build-vs-buy
          </li>
          <li>
            Miller, Evan. “How Not to Run an A/B Test”, https://www.evanmiller.org/how-not-to-run-an-ab-test.html
          </li>
        </ol>
      </div>
    </div>
  </div>

  <div id="presentation" class="main-section">
    <div class="bg-gray">
      <h2>Presentation</h2>
      <iframe src="https://www.youtube-nocookie.com/embed/sEuvyCopWZU?rel=0&showinfo=0" title="YouTube video player"
        frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
        allowfullscreen></iframe>
    </div>
  </div>

  <div id="our-team" class="main-section">
    <div>
      <div>
        <div>
          <h2>Meet our team</h2>
          <p class="text-xl text-gray-300">
            We are currently looking for opportunities. If you liked what you
            saw and want to talk more, please reach out!
          </p>
        </div>
        <ul class="people">
          <li class="profile">
            <img class="mx-auto h-40 w-40 rounded-full xl:w-56 xl:h-56 lazy" data-src="images/team/julia.jpg" alt="" />
            <div>
              <div>
                <h3>Julia Martin</h3>
                <p>Seattle, WA</p>
              </div>

              <ul class="social">
                <li>
                  <a href="mailto:juliadmartin720@gmail.com" target="_blank"><i class="fas fa-envelope"></i></a>
                </li>
                <li>
                  <a href="https://www.linkedin.com/in/juliadmartin/" target="_blank"><i
                      class="fab fa-linkedin"></i></a>
                </li>
                <li>
                  <a href="https://github.com/julia-martin" target="_blank"><i class="fab fa-github"></i></a>
                </li>
                <li>
                  <a href="https://juliamartin.dev" target="_blank"><i class="fas fa-globe"></i></a>
                </li>
              </ul>
            </div>
          </li>

          <li class="profile">
            <img class="mx-auto h-40 w-40 rounded-full xl:w-56 xl:h-56 lazy" data-src="images/team/sean.jpg" alt="" />
            <div>
              <div>
                <h3>Sean Richardson</h3>
                <p>Washington, DC</p>
              </div>

              <ul class="social">
                <li>
                  <a href="mailto:sean.richardson.umd18@gmail.com" target="_blank"><i class="fas fa-envelope"></i></a>
                </li>
                <li>
                  <a href="https://www.linkedin.com/in/sean-richardson-093212ba/" target="_blank"><i
                      class="fab fa-linkedin"></i></a>
                </li>
                <li>
                  <a href="https://github.com/seanrichardson95" target="_blank"><i class="fab fa-github"></i></a>
                </li>
                <li>
                  <a href="https://seanrichardson.dev/" target="_blank"><i class="fas fa-globe"></i></a>
                </li>
              </ul>
            </div>
          </li>

          <li class="profile">
            <img class="mx-auto h-40 w-40 rounded-full xl:w-56 xl:h-56 lazy" data-src="images/team/caleb.png" alt="" />
            <div>
              <div>
                <h3>Caleb Smith</h3>
                <p>Utah</p>
              </div>

              <ul class="social">
                <li>
                  <a href="mailto: smithcaleb042@gmail.com" target="_blank"><i class="fas fa-envelope"></i></a>
                </li>
                <li>
                  <a href="https://www.linkedin.com/in/caleb-r-smith-855071171/" target="_blank"><i
                      class="fab fa-linkedin"></i></a>
                </li>
                <li>
                  <a href="https://github.com/calebrs" target="_blank"><i class="fab fa-github"></i></a>
                </li>
                <li>
                  <a href="https://calebrs.github.io/" target="_blank"><i class="fas fa-globe"></i></a>
                </li>
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <script src="javascripts/script.js"></script>
</body>
