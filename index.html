<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />

  <!-- BEGIN Info -->
  <meta name="description"
    content="Waypost - An open-source feature flag management system that specializes in A/B Testing" />
  <meta name="title" property="og:title" content="Waypost" />
  <meta property="og:type" content="Website" />
  <meta name="image" property="og:image" content="images/thumb.png" />
  <meta name="description" property="og:description"
    content="Waypost - An open-source feature flag management system that specializes in A/B Testing" />
  <meta name="author" content="Waypost" />
  <!-- END Info -->
  <!-- BEGIN favicon -->
  <link rel="apple-touch-icon" sizes="180x180" href="images/favicon/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="images/favicon/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="images/favicon/favicon-16x16.png" />
  <link rel="manifest" href="images/favicon/site.webmanifest" />
  <link rel="shortcut icon" href="images/favicon/favicon.ico" />
  <meta name="msapplication-TileColor" content="#ffffff" />
  <meta name="msapplication-config" content="images/favicon/browserconfig.xml" />
  <meta name="theme-color" content="#ffffff" />
  <!-- END favicon -->

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Waypost</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" />
  <link rel="stylesheet" href="stylesheets/reset.css" />
  <link rel="stylesheet" href="stylesheets/style.css" />
  <link rel="stylesheet" href="stylesheets/responsive.css" />
</head>

<body>
  <header class="mobile-menu-closed">
    <div id="header">
      <a href="/">
        <img src="images/logo/Waypost_graphic_color.svg" />
        <h1>Waypost</h1>
      </a>
      <nav>
        <a href="#start-here" class="selected">Start Here</a>
        <a href="#case-study">Case Study</a>
        <a href="#presentation">Presentation</a>
        <a href="#our-team">Our Team</a>
        <a href="/documentation">Docs</a>
        <a href="https://github.com/waypost-io" target="_blank" class="icon"><i class="fab fa-github"></i></a>
      </nav>
      <div id="menu">
        <button type="button">
          <svg id="mobile-open" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"
            aria-hidden="true">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16" />
          </svg>
          <svg id="mobile-close" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"
            stroke="currentColor" aria-hidden="true">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
          </svg>
        </button>
      </div>
    </div>
    <div id="header-buffer"></div>
    <div id="mobile-menu">
      <a href="#start-here" class="selected">Start Here</a>
      <a href="#case-study">Case Study</a>
      <a href="#presentation">Presentation</a>
      <a href="#our-team">Our Team</a>
      <a href="/documentation">Docs</a>
      <a href="https://github.com/waypost-io" target="_blank"><i class="fab fa-github"></i> GitHub</a>
    </div>
  </header>

  <div id="start-here" class="main-section">
    <div class="h-full">
      <div class="bg-offwhite static-logo-color"></div>
      <div class="bg-black">
        <h4 id="hero-text">
          <span class="text-skyblue">Waypost</span> is an open-source <span class="text-skyblue">feature flagging</span>
          platform specializing in <span class="text-skyblue">A/B testing</span>
        </h4>
      </div>
    </div>
    <div class="h-full">
      <div class="bg-violet static-logo-dark">
        <h2>Easily Toggle and Test New Features</h2>
      </div>
      <div class="bg-violet">
        <h2 class="sm-header">Easily Toggle and Test New Features</h2>
        <p>
          Manage feature flags and experiments with a couple clicks.
        </p>
        <p>Waypost analyzes the data and provides experiment results.</p>
        <img src="./images/diagrams/5.2_flags_dashboard.gif" alt="Flags Dashboard" />
      </div>
    </div>
    <div class="h-full">
      <div class="bg-turquoise static-logo-dark">
        <h2>Customizable and Flexible</h2>
      </div>
      <div class="bg-turquoise">
        <h2 class="sm-header">Modular and Flexible</h2>
        <p>Waypost comes Dockerized for simple deployment.</p>
        <p>Completely open-source and self-hosted, so it can be customized according to your needs.</p>
        <p>Provides real-time updates and is easily scalable.</p>
        <img src="./images/diagrams/architecture_with_bg.png" alt="Architecture" />
      </div>
    </div>
  </div>

  <aside id="toc">
    <ul>
      <!-- Section 1 -->
      <li data-section="section-1" class="selected">
        <a href="#section-1">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Introduction</p>
          </div>
        </a>
      </li>
      <li data-section="section-1" class="subitem">
        <a href="#section-1-1">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Hypothetical Waypost User</p>
          </div>
        </a>
      </li>
      <li data-section="section-1" class="subitem">
        <a href="#section-1-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Comparing Two Versions</p>
          </div>
        </a>
      </li>
      <!-- Section 2 -->
      <li data-section="section-2">
        <a href="#section-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>A/B Testing</p>
          </div>
        </a>
      </li>
      <li data-section="section-2" class="subitem">
        <a href="#section-2-1">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>What is A/B Testing</p>
          </div>
        </a>
      </li>
      <li data-section="section-2" class="subitem">
        <a href="#section-2-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Interpreting Results</p>
          </div>
        </a>
      </li>
      <li data-section="section-2" class="subitem">
        <a href="#section-2-3">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Challenges of A/B Testing</p>
          </div>
        </a>
      </li>
      <!-- Section 3 -->
      <li data-section="section-3">
        <a href="#section-3">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Implementing Two Versions of a Site</p>
          </div>
        </a>
      </li>
      <li data-section="section-3" class="subitem">
        <a href="#section-3-1">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Multiple Deployments</p>
          </div>
        </a>
      </li>
      <li data-section="section-3" class="subitem">
        <a href="#section-3-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Feature Flags</p>
          </div>
        </a>
      </li>
      <!-- Section 4 -->
      <li data-section="section-4">
        <a href="#section-4">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Existing Solutions</p>
          </div>
        </a>
      </li>
      <li data-section="section-4" class="subitem">
        <a href="#section-4-1">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>DIY</p>
          </div>
        </a>
      </li>
      <li data-section="section-4" class="subitem">
        <a href="#section-4-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Paid Solutions</p>
          </div>
        </a>
      </li>
      <!-- Section 5 -->
      <li data-section="section-5">
        <a href="#section-5">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Waypost</p>
          </div>
        </a>
      </li>
      <li data-section="section-5" class="subitem">
        <a href="#section-5-1">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Architecture Overview</p>
          </div>
        </a>
      </li>
      <li data-section="section-5" class="subitem">
        <a href="#section-5-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Manager App</p>
          </div>
        </a>
      </li>
      <li data-section="section-5" class="subitem">
        <a href="#section-5-3">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Flag Provider Service</p>
          </div>
        </a>
      </li>
      <li data-section="section-5" class="subitem">
        <a href="#section-5-4">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>SDK</p>
          </div>
        </a>
      </li>
      <li data-section="section-5" class="subitem">
        <a href="#section-5-5">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>How Flag Data is Sent From the Manager to SDKs</p>
          </div>
        </a>
      </li>
      <li data-section="section-5" class="subitem">
        <a href="#section-5-6">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>A/B Testing with Waypost</p>
          </div>
        </a>
      </li>
      <!-- Section 6 -->
      <li data-section="section-6">
        <a href="#section-6">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Engineering Decisions</p>
          </div>
        </a>
      </li>
      <li data-section="section-6" class="subitem">
        <a href="#section-6-1">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Hosted vs. Self-Hosted</p>
          </div>
        </a>
      </li>
      <li data-section="section-6" class="subitem">
        <a href="#section-6-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Collecting User Event Data</p>
          </div>
        </a>
      </li>
      <li data-section="section-6" class="subitem">
        <a href="#section-6-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Communication Between Manager App and Flag Provider</p>
          </div>
        </a>
      </li>
      <li data-section="section-6" class="subitem">
        <a href="#section-6-3">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Providing Feature Flag Data to Client</p>
          </div>
        </a>
      </li>
      <li data-section="section-6" class="subitem">
        <a href="#section-6-4">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Providing Feature Flag Data to the SDK</p>
          </div>
        </a>
      </li>
      <li data-section="section-6" class="subitem">
        <a href="#section-6-5">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Statistics Pipeline</p>
          </div>
        </a>
      </li>
      <!-- Section 7 -->
      <li data-section="section-7">
        <a href="#section-7">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Future Work</p>
          </div>
        </a>
      </li>
      <li data-section="section-7" class="subitem">
        <a href="#section-7-1">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Extended Database Integration</p>
          </div>
        </a>
      </li>
      <li data-section="section-7" class="subitem">
        <a href="#section-7-2">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Separate Feature Flags by Application</p>
          </div>
        </a>
      </li>
      <li data-section="section-7" class="subitem">
        <a href="#section-7-3">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Login Capability</p>
          </div>
        </a>
      </li>
      <li data-section="section-7" class="subitem">
        <a href="#section-7-4">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Additional Language Support for SDKs</p>
          </div>
        </a>
      </li>
      <!-- Section 8 -->
      <li data-section="section-8">
        <a href="#section-8">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>Glossary</p>
          </div>
        </a>
      </li>
      <!-- Section 9 -->
      <li data-section="section-9">
        <a href="#section-9">
          <div>
            <div class="bullet">
              <div></div>
            </div>
            <p>References</p>
          </div>
        </a>
      </li>
    </ul>
  </aside>

  <div id="case-study" class="main-section">
    <div id="case-study-content">
      <div class="prose prose-xl">
        <h1>Case Study</h1>
        <!-- Case study goes here -->
        <p>Coming Soon!</p>
        <!-- Section 1 -->
        <h2 id="section-1">1. Introduction</h2>
        <p>Waypost is an open-source, lightweight, self-hosted feature flag management platform that specializes in A/B
          Testing, providing analytical insights for your experiments on both the front-end and back-end.</p>
        <p>As a team of developers starts to iterate on their application and develop new features, they’ll want to have
          confidence that the choices they’re making are benefiting the business. They’ll want to collect and analyze
          the behavior of users of their application in order to gain insights on whether their choices are helping,
          hurting, or having no effect on their bottom line. They’ll also need an easy and quick way to manage these new
          features and to turn them off if something goes wrong.</p>
        <p>By using feature flags with Waypost to manage and test their new features, developers can confidently push
          new features to their applications with the knowledge that the risk of the feature hurting their business is
          minimized.</p>
        <h3 id="section-1-1">1.1 Hypothetical Waypost User</h3>
        <p>Solar Flair is a solar panel installation company with a handful of software engineers on their staff. Their
          main way of gaining business is via their website’s landing page. As is standard in the solar panel
          installation business, their primary sales funnel starts with clicking a button called “Get a Quote Today”
          which opens up a form.</p>
        <p>In Solar Flair’s first iteration of their website, the form requires the customer to input the architectural
          data and their address in order to calculate how much sunlight their location typically gets. The developer
          team at Solar Flair sets up event tracking to try and understand how people interact with their website and
          they discover that 80% of people who click the “Get a Quote Today” button never finish the form and their
          quote never gets calculated. They get lost in step one of the funnel.</p>
        <p>The developers get together and discuss why this is happening and theorize that entering all the
          architectural data is tedious and cumbersome. They believe they can increase the percentage of people signing
          up for an appointment by using a 3rd-party software which only requires the address of the home as input and
          then calculates the quote using satellite imagery and climate statistics. However, this software costs a
          subscription fee, so the team wants to be sure that this fee is worth the cost and gets them enough new
          customers to gain a profit. In addition, since it’s a 3rd-party software/API that may get updated, they don’t
          want their entire site to go down if it gets a major update and the quote calculation doesn’t work. </p>
        <img src="./images/diagrams/1.1_feature_comparison.png" alt="feature comparison">
        <p>Solar Flair needs a way to **measure and analyze user behavior** when they use the new quote feature vs. the
          old one. In other words:</p>

        <p>1. They need to serve users either the new quote feature or the old one.</p>
        <p>2. They also need to determine whether users preferred the new quote, the old quote, or the change had no
          difference in behavior.</p>
        <p>If something were to go wrong with the new quote feature, Solar Flair would like all users to be served the
          old one as soon as possible to minimize any business losses from the bug while it gets fixed.</p>
        <h3 id="section-1-2">1.2 Comparing Two Versions</h3>
        <p>One option is to roll out the new feature temporarily and measure how the metrics change. But there are
          several problems with this:</p>
        <p>1. The new feature hasn’t been tested in production and could have bugs. So Solar Flair wouldn’t want it
          rolled out to everyone at once.</p>
        <p>2. This strategy doesn’t control for external factors, like time. For example, users may be more likely to
          sign up at different times of the year. Without controlling for external factors, the change in metrics might
          be attributed to the new feature when it was actually caused by something else.</p>
        <p>Therefore, Solar Flair needs to be sure the only difference between the two user experiences is the different
          feature being served to them.To do this, they’ll have to show both features during the same time period and
          randomize who gets which feature.</p>
        <p>The solution Solar Flair is looking for is experimentation, also known as A/B testing.</p>
        <h2 id="section-2">2. A/B Testing</h2>
        <h3 id="section-2-1">2.1 What is A/B Testing?</h3>
        <p>A/B testing is the practice of testing two or more versions of a feature at once, in which a random group of
          users is assigned to receive one version, and another random group of users is assigned the other version, for
          a certain period of time. The A/B test can be considered the most basic kind of randomized controlled
          experiment. In its simplest form, there are two treatments and one acts as the control for the other [1].</p>
        <img src="./images/diagrams/2.1_A_and_b_features.png" alt="A and B features">
        <h3 id="section-2-2">2.2 Interpreting Results</h3>
        <p>After an A/B test has finished running, one can analyze the results to make a data-driven decision. Every
          experiment should have a primary metric, and possibly secondary metrics, by which it will be evaluated. The
          aggregate values for these metrics get collected for the test group and the control group for comparison. One
          can use statistics to determine if there was a statistically significant change in the metrics or not.
          Statistical significance is determined by a statistic called a “p-value”. With this information, the team can
          conclude whether the new feature being tested was “successful” or not, and make a decision whether to rollout
          the feature to their entire userbase, to remove it, or to continue to run more experiments. For example, if
          Solar Flair obtained a statistically significant increase in form completion rate with their new quote
          feature, then they could confidently roll out the new feature.</p>
        <img src="./images/diagrams/2.2_AB_decision.png" alt="A/B decision">
        <h3 id="section-2-3">2.3 Challenges of A/B Testing</h3>
        <p>As A/B testing requires a large amount of data to work, implementing A/B testing comes with several
          challenges. There are two primary categories that the challenges of A/B testing fall under: engineering
          challenges and statistics challenges. </p>
        <h4>2.3.1 Engineering Challenges</h4>
        <p>From an engineering standpoint, developers must contend with the challenges of: </p>
        <p>How to assign users randomly into groups? Users must be randomly assigned so that the two groups can be
          fairly compared, and so the two groups can be roughly the same size. </p>
        <p>How to keep track of what group they were in? Each user must be given the same treatment on each visit.</p>
        <p>How to log user events? Each time an event that we are interested in occurs, it must be logged to the
          database along with relevant information regarding who, what, and where. In addition, one must log when a user
          is exposed to an experiment. This will result in a huge volume of writes to the database.</p>
        <p>How to pull the metrics to analyze afterward? Each metric is calculated differently and some may rely on the
          same events, requiring the event data to be queryable and accessible. The metric calculation should also be
          automated so that one does not have to manually query the metrics each time.</p>
        <h4>2.3.2 Statistical Challenges</h4>
        <p>From a statistics standpoint, the main challenges are:</p>
        <p>Designing the experiment correctly such that we can gain meaningful results from it. It may be surprising how
          easy it is to design an experiment incorrectly. For example, when running multiple experiments at the same
          time, one must ensure that different experiments do not influence each other’s results.</p>
        <p>Determining which statistical tests to run on the data. There are many different types of statistical tests
          to choose from, each with different purposes. Therefore, picking which statistical tests to run must be done
          thoughtfully and with care.</p>
        <h2 id="section-3">3. Implementing Two Versions of a Site</h2>
        <p>Before Solar Flair can place their users into “test” and “control” groups, they must be able to serve the
          groups different versions of their website at the same time. There are two common ways to implement this:</p>
        <p>1. Have multiple deployments of the website running and route users to one or the other.</p>
        <p>2. Use feature flags to render their site one way or the other at run time.</p>
        <h3 id="section-3-1">3.1 Possible Solution: Multiple Deployments</h3>
        <p>This strategy is implemented by having two versions of an application running in production at once. The new
          version of the application is deployed to a set percentage of the organization’s servers. A router (usually a
          load balancer) will route users to the two sets of servers. The “rollout percentage,” the percentage of the
          users get routed to one group vs. the other, can be set to any percentage the user wants, whether it be 50/50,
          10/90, etc.</p>
        <img src="./images/diagrams/3.1_multiple_deployments.png" alt="multiple deployments">
        <p>From here, if the new version proves to be an improvement, then the new version of the application is pushed
          to all the servers. If instead it has a negative effect on business metrics, then all servers with the new
          version are rolled back.</p>
        <p>This approach allows an organization to run two versions of their website, collect user data on both
          versions, and compare the results. However, there are some trade-offs to this strategy.</p>
        <h4>3.1.1 Trade-off #1: Full redeploy to fix bugs</h4>
        <p>Using multiple deployments for serving two versions of a website requires a full take-down and redeploy to
          fix a bug. If there’s a critical bug in the new version of the application, the following steps must be
          completed:</p>
        <p>1. The rollout percentage must first be set to 0% so all requests will be handled by the group of machines
          with the old version of the application.</p>
        <p>2. The machines serving the new version must be rolled-back to handle traffic without crashing.</p>
        <p>3. The load balancer must be updated to allow rolled-back servers to receive requests again.</p>
        <p>4. Once the bug is fixed, the new version must be deployed, the rollout percentage must be set and the router
          must be configured accordingly.</p>
        <p>Many of these steps can be automated, but if the organization is not using a 3rd-party, they must spend
          resources to do that. For a small organization with minimal resources, this is undesirable.</p>
        <h4>3.1.2 Trade-off #2: Complexity when testing multiple features</h4>
        <p>As discussed earlier, if two versions of a feature are being served with multiple deployments means there are
          two versions of the entire application running in production. If another feature is also being tested at that
          time, that means another version of the application is needed to test it assuming that the two features being
          tested do not overlap.</p>
        <p>For example, if Solar Flair also wants to test out a new contact form that is hosted on a separate page from
          their quote form, then those two new features would not overlap. If instead they wanted to test changing the
          form’s submit button and wanted to see how each combination of the form and button affected user behavior,
          they’d need four total deployments.</p>
        <img src="./images/diagrams/3.1.2_multivariate_testing.png" alt="multivariate testing">
        <p>Regardless of whether the additional new features being tested are overlapping or not, more tests results in:
        </p>
        <p>More deployed copies of an application.</p>
        <p>More versions of an application to keep track of, which can lead to human errors, such as implementing a
          change on the wrong version of the application.</p>
        <p>A large company with a large infrastructure and some DevOps engineers may not be be bothered by additional
          deployments of code and keeping track of the different versions of the application. But it would be more
          challenging for smaller organizations to implement this solution.</p>
        <p>Solar Flair only has a handful of developers and servers. Therefore it would not be wise to implement this
          strategy given the trade-offs. Fortunately for them, there’s a way to serve users two different versions of a
          site with just one instance of an application, through using feature flags.</p>
        <h3 id="section-3-2">3.2 Possible Solution: Feature Flags</h3>
        <p>“A feature flag is a software development process used to enable or disable functionality remotely without
          deploying code” [2].</p>
        <p>A feature flag is conditional logic connected to a remote service that can alter its flow without a
          redeployment. One can think of a feature flag as a toggle that determines whether a feature is turned “on” or
          “off.” If the flag is turned “on”, the conditional returns true and the user receives one version of the
          application. If the flag is turned “off”, the conditional returns false and they receive another.</p>
        <img src="./images/diagrams/3.2_flag_evaluation.png" alt="Flag evaluation">
        <!-- Code snippet here -->
        <p>One can wrap as much or as little code in the feature flag as they like. It could be something big like a
          completely different UI, or something small, like different colors on a “Checkout” button.</p>
        <p>Feature flags can be static, meaning they’re either “on” or “off” for everyone; or dynamic meaning that
          they’re “on” or “off” for some users based on a criteria, like whether they’re in a “control group” or a “test
          group” of an A/B test. In order to make a flag dynamic, a “Toggle Router” is needed [3]. </p>
        <h4>3.2.1 Toggle Router</h4>
        <p>A toggle router is simply something that “can be used to dynamically control which codepath is live” [3]. The
          evaluateFlag function from the example above is a toggle router. One can implement toggle routers in many
          ways, like a simple in-memory store or a standalone app with a user interface (UI).</p>
        <p>A toggle router can be customized to evaluate a flag given any desired criteria. Using a toggle router, Solar
          Flair could set a rollout percentage so that a set percentage of users receive the new quote form while the
          rest receive the old one.</p>
        <p>For example, Jack and Jill are two potential customers, both looking for solar panel installations for their
          respective homes. They each visit solarflair.net in their browsers. A toggle router inputs their unique
          identifiers (like their IP address) into a hashing algorithm and determines that Jack will receive the old
          version of the quote form and Jill will receive the new version. All this happens using just one version of
          Solar Flair’s website.</p>
        <img src="./images/diagrams/3.2.1_jack_and_jill.png" alt="jack and jill comparison">
        <h4>3.2.2 - Using Feature Flags for A/B Testing</h4>
        <p>As mentioned earlier, two key challenges of implementing AB Testing are:</p>
        <p>1. Assigning users randomly into groups.</p>
        <p>2. Serving a user the same treatment on each visit.</p>
        <p>Solar Flair can solve these two challenges by using feature flags with a toggle router.</p>
        <h2 id="section-4">4. Existing Solutions</h2>
        <p>Solar Flair now knows they want to integrate feature flags into their web app and use them to perform A/B
          tests. What they don’t know is exactly how to implement this new system. Do they want to build it from scratch
          or integrate with an existing solution? If they want to build it, how much time and effort is needed to
          accomplish that?</p>
        <h3 id="section-4-1">4.1 DIY</h3>
        <p>If they decide to build the system, they’ll need to accomplish three things:</p>
        <p>1. Persistently store their feature flag data.</p>
        <p>2. Connect the flag data to their application to dynamically serve the two versions of their site.</p>
        <p>3. Collect user event data and analyze it to gain insights to drive their decision making.</p>
        <h4>4.1.1 Config File</h4>
        <p>The easiest and simplest way to store all the flag data in one place is by using a configuration file. This
          file would contain the flag data in a data structure and could look something like the code below:</p>
        <!-- Code snippet -->
        <p>The trade-offs with the config file are:</p>
        <p>Flags can’t be set dynamically, they’re “on” or “off” for all users.</p>
        <p>Every time the config file is updated the application must be redeployed for the changes to take effect.</p>
        <p>The config file’s shortcomings make this option unappealing.</p>
        <h4>4.1.2 Database</h4>
        <img src="./images/diagrams/4.1.2_application_to_database.png" alt="applicaiton to database">
        <img src="./images/diagrams/4.1.2_flag_in_database_table.png" alt="application to database table">
        <p>A slightly better option for storing flag data is by using a database. With this configuration, Solar Flair’s
          app would query the database each time a flag’s status is needed and updates to the flags would be made using
          SQL commands, which would be cumbersome for non-technical employees to manage feature flags. The main benefit
          of this architecture is it would allow flag data to be updated without redeployment. [4]</p>
        <p>However, it still doesn’t solve the core problem of dynamically evaluating the flags. In addition, neither of
          these two options address the analysis of the data. What Solar Flair needs is something bigger and more
          robust. A place to easily manage their flags and their experiments. What they need is a feature flag
          management service, specifically one that can handle A/B testing.</p>
        <h4>4.1.3 Building a Feature Flag Management Service</h4>
        <p>Feature flag management services are platforms that allow developers to view all their feature flags in one
          place, usually through a user interface. They make the process of performing CRUD operations on the flags
          easier to implement and keep track of. In addition, the services often contain tools, such as custom flag
          evaluations, allowing developers to get more out of their feature flags.</p>
        <p>To build this system oneself, one will need to build both the feature flagging and A/B testing services. One
          must first design and build the architecture to manage and evaluate feature flags. This will require several
          components such as a frontend, backend, database, and SDK at a minimum. </p>
        <p>1. A **frontend interface** will enable both developers and non-technical members of the organization to
          easily manage feature flags and experiments.</p>
        <p>2. A **backend** server and database will enable persistent storage of data.</p>
        <p>3. An **SDK** that lives in the organization’s app should return the “on” or “off” status for a given user
          based on factors like rollout percentage. It is typically necessary to have SDKs on both the client and
          server, since there are features to be managed on both.</p>
        <p>4. The SDKs also need to receive **updated feature flag data** when a change is made from the frontend,
          ideally in real-time so that features can be shut off immediately if needed.</p>
        <img src="./images/diagrams/4.1.3_diy_architecture.png" alt="diy architecture">
        <h4>4.1.4 Building the A/B Testing Functionality</h4>
        <p>In addition to the infrastructure for feature flag management, one must also build the A/B testing
          functionality to gain insights from changes in user event data. There are 4 steps to this:</p>
        <p>1. Collect the user event data.</p>
        <p>2. Process the data.</p>
        <p>3. Run statistical analysis.</p>
        <p>4. Display the results.</p>
        <p>For the first step, a system must be in place to log user events and save them to a database using ETL
          pipelines. This infrastructure can be built in-house, or a 3rd party service like Mixpanel can be used.
          However, using a 3rd party service will offer less flexibility.</p>
        <p>Then, the data must be processed by querying the event data and calculating metrics for each experiment and
          treatment.</p>
        <p>Next, the metrics are used as inputs in statistical tests to determine whether the change in them is likely
          due to the change in feature or to random chance.</p>
        <p>Lastly, the results are displayed, ideally with some visualization.</p>
        <p>Having a data scientist on a team to give guidance would make the last three steps go smoothly. For example,
          a data scientist would know which statistical tests to run depending on the data types, how the metrics need
          to be measured and aggregated, and which kinds of visuals are most helpful. Without someone with that
          knowledge, a team may have trouble building an A/B testing solution.</p>
        <h4>4.1.5 Trade-offs</h4>
        <p>Building your own platform for feature flagging and experimentation comes with several benefits:</p>
        <p>Flexibility over how it is deployed, and ability to customize what features to include and how they are
          implemented. For example, the team could choose whether to use classical statistics or Bayesian statistics for
          the experiment analysis.</p>
        <p>Keep data in-house, reducing concerns around privacy and security</p>
        <p>However, this approach also comes with a set of trade-offs:</p>
        <p>Time and resources to build the feature flag management and A/B testing platform</p>
        <p>Responsibility to maintain the system into the future</p>
        <p>As we saw earlier, building an A/B testing platform completely from scratch is extremely hard and time
          consuming (expect at least 2,000 hours of work to get something decent) [5]. In other words, building the
          platform yourself is not free. </p>
        <p>One must also consider the accuracy and reliability of the system. Maintaining reliable analytics pipelines
          is more difficult than one may think [6]. Therefore, as the ones who created the system, the team will also be
          responsible for maintaining it into the future.</p>
        <h3 id="section-4-2">4.2 Paid Solutions</h3>
        <p>The other option is to use a 3rd party feature flagging and A/B testing platform that handles the engineering
          aspects for you so that your team does not have to invest time into building and maintaining this system. Some
          well-known players are LaunchDarkly and Optimizely. Both are feature-rich and reliable. LaunchDarkly is a
          popular feature flag management platform that offers a wide range of features, such as A/B testing, targeting
          users by attribute, integrations with productivity tools, workflow automation, and a support team. Optimizely
          is a similar platform which focuses primarily on A/B testing. However, there are several trade-offs that come
          with this approach:</p>
        <p>Since they are built by an external team, there is not as much flexibility for customization as an in-house
          platform. They would not be able to change what features to include and their implementation, or the types of
          statistical tests to use.</p>
        <p>Using one of these services entails letting a third party **store** user data, which some business may not be
          comfortable with or legally allowed to.</p>
        <p>They typically cost a monthly fee and can be out of budget for smaller companies. </p>
        <h2 id="section-5">5. Waypost</h2>
        <p>If Solar Flair was a bigger company with more engineers, then building their own system could be a good
          option. If they wanted a large set of features quickly and had a larger budget, then a paid solution like
          LaunchDarkly would be a good choice. However, the DIY option costs too much time and resources, and the paid
          option is too expensive for them and doesn’t allow them to customize the functionality. They also don’t need
          many extra features, as they only need to run simple experiments.</p>
        <p>Therefore, there was an opportunity to create a feature flagging platform that fits the needs of small
          businesses like Solar Flair. We created Waypost, a feature flag management platform that specializes in A/B
          Testing. Waypost is self-hosted and open-source, so it is completely customizable while still containing the
          core features our target user needs.</p>
        <img src="./images/diagrams/5_competitor_comparison.png" alt="competitor comparison">
        <h3 id="section-5-1">5.1 Architecture Overview</h3>
        <p>Waypost provides a feature flag and A/B testing solution through the integration of it’s own components
          (colored blue) and some existing infrastructure (colored red). The existing infrastructure is the application
          that is using feature flags and a PostgreSQL database for storing user event data, which will be referred to
          as the “Events DB”. For example, Solar Flair’s “Application” would be their web application that renders their
          website and their “Events DB” would be their existing user event database.</p>
        <img src="./images/diagrams/5.1_waypost_architecture.png" alt="Waypost architecture">
        <p>General Overview of the Components and their responsibilities:</p>
        <p>The Manager Application is responsible for managing feature flags and experiments.</p>
        <p>The Manager App sends copies of the flag data to the Flag Provider, which saves the copy and forwards it to
          each application running the Waypost SDK.</p>
        <p>The SDK is embedded into an application and is responsible for evaluating flags at run-time, and thus
          allowing one to serve multiple versions of their website to their users. It also performs the assignment of
          users into treatments.</p>
        <p>The developer using Waypost is responsible for supplying their existing user event logging solution, in which
          their **application** sends user event data to their **Events DB**.</p>
        <p>The Manager App queries the Events DB when it runs statistical analysis for the experiments and displays the
          results in the Manager’s UI.</p>
        <h3 id="section-5-2">5.2 Manager Application</h3>
        <img src="./images/diagrams/5.2_Manager_app.png" alt="Manager app">
        <p>The Manager App is what developers use to manage their feature flags and view experiment results. It also
          contains a statistics pipeline that performs the querying of data and statistical analysis for experiments.
          The Manager App has three components, a React.js application that serves as the User Interface (UI), a backend
          server built with Express.js, and a PostgreSQL database used for data persistence. </p>
        <p>The Manager App’s UI provides features for developers and non-technical members of an organization to manage
          their flags and experiments:</p>
        <h4>Flags Dashboard</h4>
        <p>The Flags Dashboard displays all the flags being used by an application. From here you can toggle, delete,
          and create flags.</p>
        <img src="./images/diagrams/5.2_flags_dashboard.gif" alt="flags Dashboard demo">
        <h4>Flag Details Page</h4>
        <p>To visit the Details Page for any flag, click on a flag’s name on the Flags Dashboard. On this page can view
          and edit the name, description, rollout percentage of the flag.</p>
        <img src="./images/diagrams/5.2_flags_detail_page.gif" alt="flag details page">
        <p>This page also allows one to view and edit custom assignments. Custom assignments are used to ensure that a
          flag evaluates a certain way for a specific user. </p>
        <p>For example, an internal member testing Solar Flair’s new quote form would add a custom assignment so when
          they connect to Solar Flair’s website the SDK will always serve them the new version of the quote form.</p>
        <img src="./images/diagrams/5.2_custom_assignments.gif" alt="Custom assignment form">
        <h4>Create Experiment Form</h4>
        <p>From a Flag’s Details Page, one can create an experiment on a flag. The most important inputs are the
          percentage of users tested and the metrics being tested. Note, Waypost doesn’t stop experiments automatically,
          the duration input is only to help the experimenter plan.</p>
        <img src="./images/diagrams/5.2_create_experiment_form.gif" alt="Create experiment form">
        <h4>Flag Events Log</h4>
        <p>The Flag Events Log is used to keeping track of the changing state of one’s flags. It displays all flag
          events, like creation, deletion, toggling, and editing of a flag. This page is most useful for debugging,
          helping a developer make connections between bugs reported in their application and feature flags.</p>
        <img src="./images/diagrams/5.2_event_log.gif" alt="Event Log">
        <h4>SDK Key</h4>
        <img src="./images/diagrams/5.2_SDK_key.jpg" alt="SDK Key">
        <p>The Manager App’s Backend Server is responsible for:</p>
        <p>Managing API endpoints for supplying and editing flag and experiment data.</p>
        <p>Sending flag data and the SDK key to the Flag Provider.</p>
        <p>The statistics pipeline, which involves querying the Events DB for metric data at the end of an experiment
          and performing statistical analysis on it to obtain the experiment’s results.</p>
        <p>The exposures pipeline, which involves querying the Events DB once a day for exposures data and aggregating
          it. The exposure data can be used by developers to know when to stop an experiment.</p>
        <img src="./images/diagrams/5.2_manager_app_responsibilities.png" alt="Manager App Responsibilities">
        <p>The Manager App’s PostgreSQL Database stores persistent experiment and flag data. Some examples are:</p>
        <p>Data pertaining to a flag, like it’s name, status (on/off), and rollout percentage.</p>
        <p>Flag event data, like at what time a flag was toggled, created, edited, or deleted.</p>
        <p>Custom assignment data, which is used to evaluate a flag a desired way for specific users.</p>
        <p>High-level experiment data, like id of the flag an experiment is on, the metrics being measured, the amount
          of users placed in the test and control groups, and the results of the experiment.</p>
        <p>The most recent SDK key.</p>
        <p>Database connection data, such as the credentials and URL of the Events DB.</p>
        <p>Query strings used by the Manager App’s Backend to query the Events DB.</p>
        <h3 id="section-5-3">5.3 Flag Provider Service</h3>
        <p>The Flag Provider is a lightweight service that acts like a cache for flag data. It is responsible for
          providing up-to-date flag data to instances of the SDK.</p>
        <img src="./images/diagrams/5.3_flag_provider.png" alt="Flag Provider">
        <h4>5.3.1 Why make providing flags a separate service?</h4>
        <p>As mentioned previously, the SDK is responsible for evaluating flags, so it must have a copy of that flag
          data and that flag data must be updated in real time in case a feature needs to be turned off immediately. The
          Manager App, which stores the flag data, could be connected to the SDK directly to serve it flags. </p>
        <p>However, this solution isn’t very scalable because the specific functionality of managing SDK connections is
          intertwined with the Manager Application’s other duties. Scalability is important in this case, because the
          number of applications running the SDK can fluctuate significantly. </p>
        <p>A better architecture for Waypost’s use case includes the Flag Provider, a service that is only responsible
          for sending updates to the SDK in real-time. The Flag Provider is lightweight and thus easily scalable.</p>
        <h4>5.3.2 Flag Provider Scaling</h4>
        <p>The Flag Provider Service has been load tested to determine how many connections to SDK clients it can
          handle. The maximum amount of connections is important for knowing when one would need to horizontally scale
          the Flag Provider. Based on our environment, response times were extremely fast. However, connections were
          refused starting at around 318 requests per second, indicating that the Flag Provider could sufficiently
          handle around 300 SSE connections at once. An organization using Waypost would therefore need to deploy more
          instances of the Flag Provider based on how many concurrent users they may have.</p>
        <h3 id="section-5-4">5.4 SDK</h3>
        <p>Waypost provides SDK’s for React.js (client-side) and Node.js (server-side). The SDK stores copies of the
          flag data and is responsible for evaluating flags, thus it is also responsible for rendering two versions of a
          site. In addition, the SDK assigns users into treatment groups for experiments.</p>
        <img src="./images/diagrams/5.4_SDK.png" alt="Waypost SDK">
        <p>The developer can use the SDK client’s evaluateFlag(flagName) method which will return a boolean value. This
          returned value is used in the app’s code to deliver one feature or another.</p>
        <!-- Code Snippet -->
        <h4>5.4.1 How Feature Evaluation Works</h4>
        <p>There are three flag attributes the evaluateFlag takes into account when evaluating:</p>
        <p>1. The **status** of a feature flag (on or off).</p>
        <p>2. **Custom assignments** on the flag.</p>
        <p>3. **Rollout percentage** of the flag.</p>
        <img src="./images/diagrams/5.4.1_flag_evaluation_flow_chart.png" alt="Flag evaluation flow chart">
        <p>The custom assignment and rollout evaluation takes an additional input, a unique identifier (often a user
          ID). For custom assignments, the unique identifier associated with the user of the application is compared to
          the existing custom assignments on the flag.</p>
        <p>For evaluating flags with rollout percentages (which includes all flags with experiments on them), a hashing
          function takes the user’s unique identifier as input and outputs a number from 0 to 99. Then, if the hash is
          less than the rollout percentage, the flag is “on” for that user, otherwise it is “off”.</p>
        <p>The above explanation is the basis of how flags are evaluated with rollout percentages, but there’s a
          downside. If an application has multiple flags with experiments, some users would be placed in the test group
          for the majority of experiments instead the placement being more random. This is problematic because it
          increases the likelihood the one experiment would affect another. It introduces an external variable.</p>
        <p>That’s why Waypost uses an offset that’s assigned to each flag with a rollout percentage. The offset moves
          the range of hashed user IDs that a flag is “on” for. For example, without an offset, two flags (Flag #1 and
          Flag #2) with 50% rollout percentages would both be “on” for users with hashed IDs from 0-49. </p>
        <img src="./images/diagrams/5.4.1_flag_rollout_1.png" alt="flag rollout 1">
        <p>If instead Flag #1 is given an offset of 30 and Flag #2 is given an offset of 5, then hashed IDs from 30-79
          would receive Flag #1 and 5-54 would receive Flag #2.</p>
        <img src="./images/diagrams/5.4.1_flag_rollout_1.png" alt="flag rollout 1">
        <h3 id="section-5-5">5.5 How Flag Data is Sent From Manager To SDKs</h3>
        <p>The SDK receives feature flag data from the Flag Provider Service via SSE (Server-Sent Events). </p>
        <p>When an application that contains the Waypost SDK starts running, it sends an initial GET request to the Flag
          Provider to fetch the current feature flag data.</p>
        <p>The SDK key is attached to this request and is validated by the Flag Provider before sending back a response.
          If the SDK key is invalid, the data will not be returned. </p>
        <p>Whenever a change is made to a feature flag, the Manager App sends a webhook to the Flag Provider. This then
          triggers the Flag Provider to send the updated set of feature flags to the SDK via SSE.</p>
        <img src="./images/diagrams/5.5_data_flow.gif" alt="Waypost data flow">
        <h3 id="section-5-6">5.6 How A/B Testing Works with Waypost</h3>
        <p>A/B Testing with Waypost involves three components, the Application (with the Waypost SDK within it), the
          Events DB, and the Manager App.</p>
        <p>The Application places users in treatments and sends user event data to the Events DB.</p>
        <p>Developers manage experiments in the Manager App’s UI. This includes creating metrics for their experiments,
          creating and stopping experiments, and viewing the results.</p>
        <p>The Manager App’s Backend queries the Events DB for exposures and metric data and performs statistical
          analysis, and stores the results of the analysis in its own database.</p>
        <img src="./images/diagrams/5.6_user_event_diagram.png" alt="User event data diagram">
        <h4>5.6.1 Events DB</h4>
        <p>In order to perform an A/B test with Waypost, the developers must have a database (that we call the Events
          DB) set up. The Events DB must contain their app’s user event data (e.g., clicks, impressions, conversions).
          This database must also contain a table that stores which treatment each user is assigned to when they are
          exposed to an experiment.</p>
        <h4>5.6.2 Connecting Manager App to Events DB</h4>
        <p>Before starting an experiment, the developer must connect the Events DB to the Manager App. They also must
          create the metrics they want to measure in the experiment.</p>
        <p>To connect the Events DB and Manager App:</p>
        <p>1. The Events DB must give read access to the Manager App</p>
        <p>2. The developer must input the Events DB credentials and a query string into the database connection form in
          the UI (shown above). The query string is used to query the Events DB for exposures and must return data in
          the schema shown below.</p>
        <img src="./images/diagrams/5.6.2_experiments_table.jpg" alt="Experiemnts table">
        <img src="./images/diagrams/5.6.1_database_connection_form.gif" alt="database connection form">
        <h4>5.6.3 Metrics</h4>
        <p>To add metrics, the developer must provide:</p>
        <p>The name</p>
        <p>The type of metric</p>
        <p>A query string, which is used to query the Events DB</p>
        <img src="./images/diagrams/5.6.3_new_metric_page.gif" alt="New metrics page">
        <p>After the Manager App confirms it can query the Events DB to obtain a metric, the developer can attach that
          metric to an experiment. Some common examples of metrics are:</p>
        <p>Click-through rate</p>
        <p>Signup rate</p>
        <p>Time spent on site</p>
        <h4>5.6.4 Experiment Exposures Pipeline</h4>
        <img src="./images/diagrams/5.6.4_exposures_pipeline.png" alt="Exposures pipeline">
        <p>Once an experiment has been created, Waypost aggregates and analyzes this data. The Experiment Exposures
          Pipeline runs once every night to aggregate the number of users bucketed into each cohort of the experiment,
          and stores the aggregated data into a table in the Manager App’s database. In case the pipeline ever fails, it
          also checks for and backfills any missing data each time it runs. The Manager App then displays this as a line
          chart essentially depicting the sample size of each cohort over time. The purpose of this visualization is to
          help the developer understand the sample size and determine how much longer the experiment should run.</p>
        <img src="./images/diagrams/5.6.4_exposures_graph.jpg" alt="Exposures Graph">
        <h4>5.6.5 Statistics Pipeline</h4>
        <p>Once the developer is ready to view the results of their experiment, they can click on the “Refresh Results”
          button in the Manager App’s UI. This triggers the Statistics Pipeline, which:</p>
        <p>1. **Queries** the Events DB for the metrics being tracked for this experiment.</p>
        <p>2. **Analyzes** this data using statistical tests, using the *t-test* for continuous metrics (count,
          duration, revenue) and the *chi-squared* test for discrete metrics (binomial).</p>
        <p>3. **Stores** the results in the Waypost DB, which will be displayed in the UI as a table (shown below).</p>
        <p>Information on how to interpret the statistical results are included in our documentation site.</p>
        <img src="./images/diagrams/5.6.5_results_page.jpg" alt="Results page">
        <h2 id="section-6">6. Engineering Decisions and Trade-Offs</h2>
        <h3 id="section-6-1">6.1 Hosted vs. Self-Hosted</h3>
        <h3 id="section-6-2">6.2 Collecting User Event Data</h3>
        <h3 id="section-6-3">6.3 Communication Between Manager App and Flag Provider</h3>
        <h3 id="section-6-4">6.4 Providing Feature Flag Data to SDK</h3>
        <h3 id="section-6-5">6.5 Statistics Pipeline</h3>
        <h2 id="section-7">7. Future Work</h2>
        <h3 id="section-7-1">7.1 Extended Database Integration</h3>
        <h3 id="section-7-2">7.2 Feature Flags by Application</h3>
        <h3 id="section-7-3">7.3 Login Capability</h3>
        <h3 id="section-7-4">7.4 Additional Language Support for SDKs</h3>
        <h2 id="section-8">8. Glossary</h2>
        <ul>
          <li>
            <b>Metrics</b> - measures of quantitative assessment commonly used for assessing, comparing, and tracking
            performance or production
          </li>
          <li>
            <b>Controlled Experiment</b> - an experiment in which all the variable factors in an experimental group and
            a comparison control group are kept the same except for one variable factor in the experimental group that
            is changed or altered
          </li>
          <li>
            <b>Statistically Significant</b> - a determination that a relationship between two or more variables is
            caused by something other than chance.
          </li>
          <li>
            <b>Data Pipeline</b> - a set of actions that ingest raw data from disparate sources and move the data to a
            destination for storage and analysis <a
              href="https://www.stitchdata.com/resources/what-is-data-pipeline">[source]</a>.
          </li>
          <li>
            <b>Backfill</b> - any process that involves modifying or adding new data to existing records in a dataset
          </li>
          <li>
            <b>Continuous data</b> - data that can take any value within a range, such as a person’s weight
          </li>
          <li>
            <b>Discrete data</b> - data that is countable and distinct, and can only take certain values, such as number
            of signups.
          </li>
        </ul>
        <h2 id="section-9">9. References</h2>
        <ol>
          <li>
            Gallo, Amy. “A Refresher on A/B Testing”, https://hbr.org/2017/06/a-refresher-on-ab-testing
          </li>
          <li>
            Parzych, Dawn. “What are Feature Flags”, https://launchdarkly.com/blog/what-are-feature-flags
          </li>
          <li>
            Hodgson, Pete. “Feature Toggles (aka Feature Flags)”, https://martinfowler.com/articles/feature-toggles.html
          </li>
          <li>
            Haddad, Rowan. “Fun with Flags: Where to Store Feature Flags”, https://www.flagship.io/feature-flag-storage
          </li>
          <li>
            Dorn, Jeremy. “Open Source A/B Testing”, https://medium.com/growth-book/open-source-a-b-testing-dbc68aedab70
          </li>
          <li>
            Pam, Robin. “Build vs. Buy: Choosing the Right Experimentation Solution”,
            https://www.optimizely.com/insights/blog/build-vs-buy
          </li>
          <li>
            Miller, Evan. “How Not to Run an A/B Test”, https://www.evanmiller.org/how-not-to-run-an-ab-test.html
          </li>
        </ol>
      </div>
    </div>
  </div>

  <div id="presentation" class="main-section">
    <div class="bg-gray">
      <h2>Presentation</h2>
      <iframe src="https://www.youtube-nocookie.com/embed/sEuvyCopWZU?rel=0&showinfo=0" title="YouTube video player"
        frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
        allowfullscreen></iframe>
    </div>
  </div>

  <div id="our-team" class="main-section">
    <div>
      <div>
        <div>
          <h2>Meet our team</h2>
          <p class="text-xl text-gray-300">
            We are currently looking for opportunities. If you liked what you
            saw and want to talk more, please reach out!
          </p>
        </div>
        <ul class="people">
          <li class="profile">
            <img class="mx-auto h-40 w-40 rounded-full xl:w-56 xl:h-56 lazy" data-src="images/team/julia.jpg" alt="" />
            <div>
              <div>
                <h3>Julia Martin</h3>
                <p>Seattle, WA</p>
              </div>

              <ul class="social">
                <li>
                  <a href="mailto:juliadmartin720@gmail.com" target="_blank"><i class="fas fa-envelope"></i></a>
                </li>
                <li>
                  <a href="https://www.linkedin.com/in/juliadmartin/" target="_blank"><i
                      class="fab fa-linkedin"></i></a>
                </li>
                <li>
                  <a href="https://github.com/julia-martin" target="_blank"><i class="fab fa-github"></i></a>
                </li>
                <li>
                  <a href="https://juliamartin.dev" target="_blank"><i class="fas fa-globe"></i></a>
                </li>
              </ul>
            </div>
          </li>

          <li class="profile">
            <img class="mx-auto h-40 w-40 rounded-full xl:w-56 xl:h-56 lazy" data-src="images/team/sean.jpg" alt="" />
            <div>
              <div>
                <h3>Sean Richardson</h3>
                <p>Washington, DC</p>
              </div>

              <ul class="social">
                <li>
                  <a href="mailto:sean.richardson.umd18@gmail.com" target="_blank"><i class="fas fa-envelope"></i></a>
                </li>
                <li>
                  <a href="https://www.linkedin.com/in/sean-richardson-093212ba/" target="_blank"><i
                      class="fab fa-linkedin"></i></a>
                </li>
                <li>
                  <a href="https://github.com/seanrichardson95" target="_blank"><i class="fab fa-github"></i></a>
                </li>
                <li>
                  <a href="https://seanrichardson.dev/" target="_blank"><i class="fas fa-globe"></i></a>
                </li>
              </ul>
            </div>
          </li>

          <li class="profile">
            <img class="mx-auto h-40 w-40 rounded-full xl:w-56 xl:h-56 lazy" data-src="images/team/caleb.png" alt="" />
            <div>
              <div>
                <h3>Caleb Smith</h3>
                <p>Utah</p>
              </div>

              <ul class="social">
                <li>
                  <a href="mailto: smithcaleb042@gmail.com" target="_blank"><i class="fas fa-envelope"></i></a>
                </li>
                <li>
                  <a href="https://www.linkedin.com/in/caleb-r-smith-855071171/" target="_blank"><i
                      class="fab fa-linkedin"></i></a>
                </li>
                <li>
                  <a href="https://github.com/calebrs" target="_blank"><i class="fab fa-github"></i></a>
                </li>
                <li>
                  <a href="https://calebrs.github.io/" target="_blank"><i class="fas fa-globe"></i></a>
                </li>
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <script src="javascripts/script.js"></script>
</body>